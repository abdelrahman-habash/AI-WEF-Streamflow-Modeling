{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-V3U3-TlYeXX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7295,
     "status": "ok",
     "timestamp": 1758846621968,
     "user": {
      "displayName": "Abdelrahman Habash",
      "userId": "17825699896667390000"
     },
     "user_tz": -180
    },
    "id": "-V3U3-TlYeXX",
    "outputId": "73635448-f38e-4959-875b-0b6bdd0b35a5"
   },
   "outputs": [],
   "source": [
    "# pip install hydroErr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m3TwcK-_aMuY",
   "metadata": {
    "id": "m3TwcK-_aMuY"
   },
   "outputs": [],
   "source": [
    "# pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ff284",
   "metadata": {
    "id": "367ff284"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import xgboost as xgb\n",
    "from keras import layers\n",
    "from keras import initializers\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification, make_blobs\n",
    "from matplotlib.colors import ListedColormap\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import HydroErr as he\n",
    "from matplotlib import rcParams\n",
    "import pickle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import shap\n",
    "import seaborn as sns\n",
    "import re\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tqdm import tqdm\n",
    "\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten, Conv1D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import Huber\n",
    "\n",
    "import gc\n",
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a964f-efad-424e-b061-6a1ad4f779e6",
   "metadata": {
    "id": "7e1a964f-efad-424e-b061-6a1ad4f779e6"
   },
   "outputs": [],
   "source": [
    "def compute_metrics(true_series, forecast):\n",
    "    true_series = np.array(np.round(true_series, 12))\n",
    "    forecast = np.array(np.round(forecast, 12))\n",
    "\n",
    "    # RÂ²\n",
    "    R2 = 1 - ((np.sum((true_series - forecast) ** 2)) / (np.sum((true_series - np.mean(true_series)) ** 2)))\n",
    "\n",
    "    # Nash-Sutcliffe Efficiency\n",
    "    NSE = he.nse(true_series, forecast)\n",
    "\n",
    "    # RMSE\n",
    "    RMSE = np.sqrt(np.mean((true_series - forecast) ** 2))\n",
    "\n",
    "    # Percent Bias (PBIAS)\n",
    "    Pbias_num = np.sum(true_series - forecast)\n",
    "    Pbias_denom = np.sum(true_series)\n",
    "    Pbias = (Pbias_num / Pbias_denom) * 100\n",
    "\n",
    "    return R2, NSE, RMSE, Pbias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66e63e4-91fc-4b2c-b5bd-e8911f14e323",
   "metadata": {
    "id": "d66e63e4-91fc-4b2c-b5bd-e8911f14e323"
   },
   "outputs": [],
   "source": [
    "# Set font properties\n",
    "rcParams.update({\n",
    "    'font.size': 12,          # Default text size\n",
    "    # 'font.family': 'serif',   # Font family\n",
    "    # 'font.serif': ['Cambria'],  # Specific serif font  ['Times New Roman', Palatino', 'Garamond', 'Cambria', 'Bookman']\n",
    "    'axes.titlesize': 14,     # Title font size\n",
    "    'axes.labelsize': 12,     # X and Y label size\n",
    "    'xtick.labelsize': 11,    # X tick label size\n",
    "    'ytick.labelsize': 11,    # Y tick label size\n",
    "    'legend.fontsize': 11,    # Legend font size\n",
    "    'figure.titlesize': 16,   # Figure title size\n",
    "    'figure.titleweight': 'bold'  # Figure title weight\n",
    "\n",
    "}),\n",
    "\n",
    "# function to plot time series and monthly averages with metrics\n",
    "def plot_series(time, y_test, y_pred, format=\"-\", start=0, end=None, title=None):\n",
    "\n",
    "    # Plot the true and predicted values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(time[start:end], y_test[start:end], format, linewidth=1, label=\"True Values\")\n",
    "    plt.plot(time[start:end], y_pred[start:end], format, linewidth=1, label=\"Predicted Values\")\n",
    "    plt.title(f\"Daily Average Streamflow - {title}\", )\n",
    "    plt.xlabel(\"Date\",  style='italic')\n",
    "    plt.ylabel(\"Streamflow (m$^3$/s)\",  style='italic')\n",
    "    # Compute metrics for the main plot and display them\n",
    "    R2i, NSEi, RMSEi, Pbiasi = compute_metrics(y_test, y_pred)\n",
    "    print(f\"Metrics for Daily Average Streamflow - {title}:\")\n",
    "    print(f\"  R2: {R2i:.5f}, NSE: {NSEi:.5f}, RMSE: {RMSEi:.5f}, Pbias: {Pbiasi:.5f}\")\n",
    "    # Add metrics as text on the plot\n",
    "    textstr = '\\n'.join((\n",
    "        f\"R$^2$: {R2i:.3f}\",\n",
    "        f\"NSE: {NSEi:.3f}\",\n",
    "        f\"RMSE: {RMSEi:.3f} m$^3$/s\",\n",
    "        f\"Pbias: {Pbiasi:.3f} %\"\n",
    "    ))\n",
    "    plt.gca().text(0.02, 0.96, textstr, transform=plt.gca().transAxes,\n",
    "                   fontsize=11, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='black', alpha=0.7),\n",
    "                   )\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # Compute monthly averages\n",
    "    df = pd.DataFrame({'time': time, 'y_test': y_test, 'y_pred': y_pred})\n",
    "    df['time'] = pd.to_datetime(df['time'])\n",
    "    df.set_index('time', inplace=True)\n",
    "    monthly_avg = df.resample('ME').mean()\n",
    "\n",
    "    # Plot the averaged monthly streamflow\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(monthly_avg.index, monthly_avg['y_test'], label=\"True Values\", linewidth=1, marker='o')\n",
    "    plt.plot(monthly_avg.index, monthly_avg['y_pred'], label=\"Predicted Values\", linewidth=1, marker='o')\n",
    "    plt.title(f\"Monthly Average Streamflow - {title}\", )\n",
    "    plt.xlabel(\"Month\",  style='italic')\n",
    "    plt.ylabel(\"Streamflow (m$^3$/s)\",  style='italic')\n",
    "\n",
    "    # Compute metrics for monthly averages\n",
    "    R2m, NSEm, RMSEm, Pbiasm = compute_metrics(monthly_avg['y_test'], monthly_avg['y_pred'])\n",
    "    print(f\"Metrics for Monthly Average Streamflow - {title}:\")\n",
    "    print(f\"  R2: {R2m:.5f}, NSE: {NSEm:.5f}, RMSE: {RMSEm:.5f}, Pbias: {Pbiasm:.5f}\")\n",
    "\n",
    "    # Add metrics for monthly averages as text on the plot\n",
    "    textstr_monthly = '\\n'.join((\n",
    "        f\"R$^2$: {R2m:.3f}\",\n",
    "        f\"NSE: {NSEm:.3f}\",\n",
    "        f\"RMSE: {RMSEm:.3f} m$^3$/s\",\n",
    "        f\"Pbias: {Pbiasm:.3f} %\"\n",
    "    ))\n",
    "    plt.gca().text(0.02, 0.96, textstr_monthly, transform=plt.gca().transAxes,\n",
    "                   fontsize=11, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', facecolor='white', edgecolor='black', alpha=0.7),\n",
    "                   )\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba20152-1bbb-4d46-8ab5-0e53707be14c",
   "metadata": {
    "id": "dba20152-1bbb-4d46-8ab5-0e53707be14c"
   },
   "outputs": [],
   "source": [
    "# Enable interactive plotting in Jupyter Notebook\n",
    "%matplotlib notebook\n",
    "# Ensure inline plotting in Jupyter Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6a6c39",
   "metadata": {
    "id": "9f6a6c39"
   },
   "source": [
    "# Fetching Climate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04da4646",
   "metadata": {
    "id": "04da4646"
   },
   "source": [
    "### ERA5-Land Climate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6b4566-4981-4e1f-9006-6abdd66a2133",
   "metadata": {
    "id": "1d6b4566-4981-4e1f-9006-6abdd66a2133"
   },
   "outputs": [],
   "source": [
    "# Folder where all the CSV files are stored\n",
    "folder_path = \"/***/***/Climate_data\"  # Replace with the actual path to the file\n",
    "\n",
    "# Initialize an empty list to hold each file's DataFrame\n",
    "all_dataframes = []\n",
    "\n",
    "# Function to safely convert date values to 'YYYY-MM-DD'\n",
    "def convert_mixed_date(val):\n",
    "    if pd.isna(val):\n",
    "        return val\n",
    "    val = str(val)\n",
    "    if re.match(r\"^\\d{8}$\", val):  # Format: YYYYMMDD\n",
    "        return pd.to_datetime(val, format='%Y%m%d').strftime('%Y-%m-%d')\n",
    "    try:\n",
    "        return pd.to_datetime(val).strftime('%Y-%m-%d')\n",
    "    except:\n",
    "        return pd.NaT\n",
    "\n",
    "# Loop through each CSV file in the folder\n",
    "for file_name in os.listdir(folder_path):\n",
    "    if file_name.endswith(\".csv\"):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "        # Read the CSV into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "\n",
    "        # Convert 'date' column format from mixed formats to YYYY-MM-DD if the 'date' column exists\n",
    "        if 'date' in df.columns:\n",
    "            df['date'] = df['date'].apply(convert_mixed_date)\n",
    "\n",
    "        # Rename columns for consistency\n",
    "        df.rename(columns={'date': 'Date'}, inplace=True)\n",
    "        df.rename(columns={'station_no': 'Station No.'}, inplace=True)\n",
    "\n",
    "        # Append the DataFrame to the list\n",
    "        all_dataframes.append(df)\n",
    "\n",
    "# Concatenate all the DataFrames into a single DataFrame\n",
    "WEF_Model_Climate_data = pd.concat(all_dataframes, ignore_index=True)\n",
    "\n",
    "# Sort the final DataFrame by Station No. and Date\n",
    "WEF_Model_Climate_data = WEF_Model_Climate_data.sort_values(by=['Station No.', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Optional: Remove unwanted columns if they exist\n",
    "WEF_Model_Climate_data.drop(columns=['system:index', '.geo'], errors='ignore', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3f6cb3-01c4-405f-b558-2d9d7d7902c9",
   "metadata": {
    "id": "2f3f6cb3-01c4-405f-b558-2d9d7d7902c9"
   },
   "outputs": [],
   "source": [
    "WEF_Model_Climate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114a7fb7",
   "metadata": {
    "id": "114a7fb7"
   },
   "outputs": [],
   "source": [
    "# Optionally, to save it to a CSV file:\n",
    "# WEF_Model_Climate_data.to_csv('WEF_Model_Climate_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdace00d-71a4-4a7e-a84f-ae9e43570577",
   "metadata": {
    "id": "fdace00d-71a4-4a7e-a84f-ae9e43570577"
   },
   "outputs": [],
   "source": [
    "Parameters_in_forecasted_dataset = [\n",
    "    'Station No.',\n",
    "    'Date',\n",
    "    'u_component_of_wind_10m',                    # 10m U-Component of Wind\n",
    "    'v_component_of_wind_10m',                    # 10m V-Component of Wind\n",
    "    'dewpoint_temperature_2m',                    # 2m Dewpoint Temperature\n",
    "    'temperature_2m',                             # 2m Temperature\n",
    "    'total_evaporation',                          # Total Evaporation\n",
    "    'runoff',                                     # Runoff\n",
    "    'surface_latent_heat_flux',                   # Surface Latent Heat Flux\n",
    "    'surface_net_solar_radiation',                # Surface Net Solar Radiation\n",
    "    'surface_net_thermal_radiation',              # Surface Net Thermal Radiation\n",
    "    'surface_runoff',                             # Surface Runoff\n",
    "    'surface_sensible_heat_flux',                 # Surface Sensible Heat Flux\n",
    "    'surface_solar_radiation_downwards',          # Surface Solar Radiation Downwards\n",
    "    'surface_thermal_radiation_downwards',        # Surface Thermal Radiation Downwards\n",
    "    'snow_depth',                                 # Snow Depth\n",
    "    'snow_density',                               # Snow Density\n",
    "    'soil_temperature_level_1',                   # Soil Temperature Level 1\n",
    "    'sub_surface_runoff',                         # Sub-Surface Runoff\n",
    "    'total_precipitation',                        # Total Precipitation\n",
    "    'volumetric_soil_water_layer_1'               # Volumetric Soil Moisture (Layer 1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29242825-4b5b-44bc-bce1-de4862eaca8f",
   "metadata": {
    "id": "29242825-4b5b-44bc-bce1-de4862eaca8f"
   },
   "outputs": [],
   "source": [
    "WEF_Model_Climate_data = WEF_Model_Climate_data[Parameters_in_forecasted_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a801be-fa9d-4be4-84bf-05d21a658f62",
   "metadata": {
    "id": "98a801be-fa9d-4be4-84bf-05d21a658f62"
   },
   "outputs": [],
   "source": [
    "WEF_Model_Climate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7787a47-a1e9-48eb-b8e2-6ec6e08aa409",
   "metadata": {
    "id": "d7787a47-a1e9-48eb-b8e2-6ec6e08aa409"
   },
   "source": [
    "### Climate Data in 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f651c1-30eb-46da-ba7f-4f90af9df41e",
   "metadata": {
    "id": "e5f651c1-30eb-46da-ba7f-4f90af9df41e"
   },
   "outputs": [],
   "source": [
    "WEF_Model_Climate_data = WEF_Model_Climate_data[WEF_Model_Climate_data['Date'] >= '2007-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7efde-75ba-4d27-9c77-e78d4898cc12",
   "metadata": {
    "id": "62e7efde-75ba-4d27-9c77-e78d4898cc12"
   },
   "outputs": [],
   "source": [
    "WEF_Model_Climate_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6ca102",
   "metadata": {
    "id": "de6ca102"
   },
   "source": [
    "# Fetching Streamflow Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1eb1b1",
   "metadata": {
    "id": "2c1eb1b1"
   },
   "source": [
    "### Streamflow stations data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06df3bf9",
   "metadata": {
    "id": "06df3bf9"
   },
   "outputs": [],
   "source": [
    "streamflow_stations = pd.read_excel('/***/***/Streamflow_stations.xlsx') # Replace with the actual path to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18abf779",
   "metadata": {
    "id": "18abf779"
   },
   "outputs": [],
   "source": [
    "streamflow_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13546c2",
   "metadata": {
    "id": "b13546c2"
   },
   "outputs": [],
   "source": [
    "WEF_Model_info_data_07 = pd.read_excel('/***/***/Streamflow_Stations_info.xlsx') # Replace with the actual path to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "386ebe7e",
   "metadata": {
    "id": "386ebe7e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "WEF_Model_info_data_07"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754df856",
   "metadata": {
    "id": "754df856"
   },
   "source": [
    "### Streamflow data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d972095d",
   "metadata": {
    "id": "d972095d"
   },
   "outputs": [],
   "source": [
    "dsi_2005_Q_Data = pd.read_csv('/***/***/dsi_2005_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2006_Q_Data = pd.read_csv('/***/***/dsi_2006_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2007_Q_Data = pd.read_csv('/***/***/dsi_2007_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2008_Q_Data = pd.read_csv('/***/***/dsi_2008_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2009_Q_Data = pd.read_csv('/***/***/dsi_2009_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2010_Q_Data = pd.read_csv('/***/***/dsi_2010_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2011_Q_Data = pd.read_csv('/***/***/dsi_2011_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2012_Q_Data = pd.read_csv('/***/***/dsi_2012_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2013_Q_Data = pd.read_csv('/***/***/dsi_2013_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2014_Q_Data = pd.read_csv('/***/***/dsi_2014_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2015_Q_Data = pd.read_csv('/***/***/dsi_2015_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2016_Q_Data = pd.read_csv('/***/***/dsi_2016_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2017_Q_Data = pd.read_csv('/***/***/dsi_2017_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2018_Q_Data = pd.read_csv('/***/***/dsi_2018_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2019_Q_Data = pd.read_csv('/***/***/dsi_2019_Q_Data.csv') # Replace with the actual path to the file\n",
    "dsi_2020_Q_Data = pd.read_csv('/***/***/dsi_2020_Q_Data.csv') # Replace with the actual path to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d009fa",
   "metadata": {
    "id": "72d009fa",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List of DataFrames\n",
    "dsi_Q_data = [\n",
    "    dsi_2005_Q_Data, dsi_2006_Q_Data, dsi_2007_Q_Data, dsi_2008_Q_Data,\n",
    "    dsi_2009_Q_Data, dsi_2010_Q_Data, dsi_2011_Q_Data, dsi_2012_Q_Data,\n",
    "    dsi_2013_Q_Data, dsi_2014_Q_Data, dsi_2015_Q_Data, dsi_2016_Q_Data,\n",
    "    dsi_2017_Q_Data, dsi_2018_Q_Data, dsi_2019_Q_Data, dsi_2020_Q_Data,\n",
    "]\n",
    "\n",
    "# Filter each DataFrame and store the result in a list\n",
    "WEF_Model_streamflow_df = [df for df in dsi_Q_data]\n",
    "\n",
    "# Concatenate the filtered DataFrames into a single DataFrame\n",
    "WEF_Model_streamflow_df = pd.concat(WEF_Model_streamflow_df)\n",
    "\n",
    "# Reset the index of the resulting DataFrame\n",
    "WEF_Model_streamflow_df.reset_index(drop=True, inplace=True)\n",
    "WEF_Model_streamflow_df['Date'] = pd.to_datetime(WEF_Model_streamflow_df['Date'], format='%d/%m/%Y')\n",
    "\n",
    "WEF_Model_stations = WEF_Model_info_data_07['Station No.'].unique()\n",
    "print(WEF_Model_stations)\n",
    "\n",
    "WEF_Model_streamflow_df = WEF_Model_streamflow_df[WEF_Model_streamflow_df['Station No.'].isin(WEF_Model_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc45f6-bd13-4c7f-a22c-dc46ce981507",
   "metadata": {
    "id": "dcfc45f6-bd13-4c7f-a22c-dc46ce981507",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsi_Q_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544d4fc0-6f12-4174-961c-53b3fce8177f",
   "metadata": {
    "id": "544d4fc0-6f12-4174-961c-53b3fce8177f"
   },
   "source": [
    "### Model stations data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a79cd4-a2e1-4dcb-aa73-98fd029af44f",
   "metadata": {
    "id": "55a79cd4-a2e1-4dcb-aa73-98fd029af44f"
   },
   "source": [
    "### streamflow data in 2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e880423c-2437-4360-a5f8-61eae7052b68",
   "metadata": {
    "id": "e880423c-2437-4360-a5f8-61eae7052b68"
   },
   "outputs": [],
   "source": [
    "WEF_Model_streamflow_data = WEF_Model_streamflow_df[WEF_Model_streamflow_df['Date'] >= '2007-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce169d6b",
   "metadata": {
    "id": "ce169d6b"
   },
   "outputs": [],
   "source": [
    "WEF_Model_streamflow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b7916c-d727-454e-b7c2-72794a97daa0",
   "metadata": {
    "id": "b9b7916c-d727-454e-b7c2-72794a97daa0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Check data types of all columns\n",
    "print(\"Column Types Before Conversion:\")\n",
    "print(WEF_Model_streamflow_data.dtypes)\n",
    "\n",
    "# Step 2: Identify non-numeric values before conversion using .loc for safe access\n",
    "non_numeric_before_conversion = WEF_Model_streamflow_data.loc[~WEF_Model_streamflow_data['Flow rate (m3/s)'].apply(lambda x: pd.to_numeric(x, errors='coerce')).notna()]\n",
    "\n",
    "# Preview non-numeric values before conversion\n",
    "print(\"Non-numeric values in 'Flow rate (m3/s)' column before conversion:\")\n",
    "print(non_numeric_before_conversion[['Station No.', 'Date', 'Flow rate (m3/s)']])\n",
    "\n",
    "# Step 3: Convert 'Flow rate (m3/s)' column to numeric using .loc to avoid warnings\n",
    "WEF_Model_streamflow_data.loc[:, 'Flow rate (m3/s)'] = pd.to_numeric(WEF_Model_streamflow_data['Flow rate (m3/s)'], errors='coerce')\n",
    "\n",
    "# Step 4: Verify the conversion by displaying any NaN values in 'Flow rate (m3/s)'\n",
    "non_convertible_after_conversion = WEF_Model_streamflow_data.loc[WEF_Model_streamflow_data['Flow rate (m3/s)'].isna(), ['Station No.', 'Date', 'Flow rate (m3/s)']]\n",
    "\n",
    "# Display non-convertible entries after conversion to confirm they match the previewed non-numeric values\n",
    "print(\"\\nNon-convertible values in 'Flow rate (m3/s)' column after conversion to NaN:\")\n",
    "print(non_convertible_after_conversion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2125593f-842f-4b95-b723-85ff6e68f145",
   "metadata": {
    "id": "2125593f-842f-4b95-b723-85ff6e68f145"
   },
   "outputs": [],
   "source": [
    "# Ensure 'Date' column is in datetime format\n",
    "WEF_Model_streamflow_data.loc[:, 'Date'] = pd.to_datetime(WEF_Model_streamflow_data['Date'])\n",
    "# List to collect the reindexed station DataFrames\n",
    "reindexed_dfs = []\n",
    "# Loop through each station and reindex its date range\n",
    "for station in WEF_Model_streamflow_data['Station No.'].unique():\n",
    "    station_df = WEF_Model_streamflow_data[WEF_Model_streamflow_data['Station No.'] == station].copy()\n",
    "    # Create complete date range for the station\n",
    "    full_date_range = pd.date_range(start=station_df['Date'].min(), end=station_df['Date'].max(), freq='D')\n",
    "    # Set 'Date' as index for reindexing\n",
    "    station_df = station_df.set_index('Date')\n",
    "    # Reindex to include all dates (missing dates will have NaNs)\n",
    "    station_df = station_df.reindex(full_date_range)\n",
    "    # Restore 'Date' as a column\n",
    "    station_df = station_df.rename_axis('Date').reset_index()\n",
    "    # Refill station number where missing\n",
    "    station_df['Station No.'] = station\n",
    "    # Append to list\n",
    "    reindexed_dfs.append(station_df)\n",
    "# Concatenate all reindexed station DataFrames\n",
    "WEF_Model_streamflow_data = pd.concat(reindexed_dfs).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591a14b5-5656-4621-84e3-e7cb79afa513",
   "metadata": {
    "id": "591a14b5-5656-4621-84e3-e7cb79afa513"
   },
   "outputs": [],
   "source": [
    "# Identify stations with NaNs before interpolation\n",
    "stations_with_nans = WEF_Model_streamflow_data[WEF_Model_streamflow_data['Flow rate (m3/s)'].isna()]['Station No.'].unique()\n",
    "\n",
    "print(len(stations_with_nans))\n",
    "print(stations_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3a3b8f-03a3-4cd3-924c-73fdf8e3f1a8",
   "metadata": {
    "id": "db3a3b8f-03a3-4cd3-924c-73fdf8e3f1a8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Ensure 'Date' column is datetime\n",
    "WEF_Model_streamflow_data.loc[:, 'Date'] = pd.to_datetime(WEF_Model_streamflow_data['Date'])\n",
    "WEF_Model_streamflow_data = WEF_Model_streamflow_data.sort_values(['Station No.', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Function to identify missing periods\n",
    "def identify_missing_periods(data, station):\n",
    "    missing_intervals = []\n",
    "    total_missing_days = 0\n",
    "    data = data.sort_values('Date').reset_index(drop=True)\n",
    "    missing_data = data[data['Flow rate (m3/s)'].isna()]\n",
    "    if missing_data.empty:\n",
    "        return missing_intervals\n",
    "    start_date = missing_data.iloc[0]['Date']\n",
    "    previous_date = start_date\n",
    "\n",
    "    for current_date in missing_data['Date'].iloc[1:]:\n",
    "        if (current_date - previous_date).days == 1:\n",
    "            previous_date = current_date\n",
    "        else:\n",
    "            missing_intervals.append((start_date, previous_date, (previous_date - start_date).days + 1))\n",
    "            total_missing_days += (previous_date - start_date).days + 1\n",
    "            start_date = current_date\n",
    "            previous_date = current_date\n",
    "\n",
    "    missing_intervals.append((start_date, previous_date, (previous_date - start_date).days + 1))\n",
    "    total_missing_days += (previous_date - start_date).days + 1\n",
    "    return missing_intervals\n",
    "\n",
    "# Function to interpolate gaps with â‰¤ 31 missing days\n",
    "def interpolate_small_gaps(data, station):\n",
    "    data = data.copy()\n",
    "    data = data.sort_values('Date').reset_index(drop=True)\n",
    "    data['Flow rate (m3/s)'] = pd.to_numeric(data['Flow rate (m3/s)'], errors='coerce')\n",
    "    missing_intervals = identify_missing_periods(data, station)\n",
    "    small_gaps = [interval for interval in missing_intervals if interval[2] <= 31]\n",
    "\n",
    "    for start, end, days in small_gaps:\n",
    "        print(f\"\\nInterpolating for Station {station}: {start.date()} to {end.date()} ({days} days)\")\n",
    "        mask = (data['Date'] >= start) & (data['Date'] <= end)\n",
    "\n",
    "        before = data[data['Date'] < start].dropna(subset=['Flow rate (m3/s)'])\n",
    "        after = data[data['Date'] > end].dropna(subset=['Flow rate (m3/s)'])\n",
    "        before_val = before.iloc[-1]['Flow rate (m3/s)'] if not before.empty else np.nan\n",
    "        after_val = after.iloc[0]['Flow rate (m3/s)'] if not after.empty else np.nan\n",
    "\n",
    "        if pd.notna(before_val) and pd.notna(after_val):\n",
    "            data.loc[mask, 'Flow rate (m3/s)'] = data['Flow rate (m3/s)'].interpolate(method='linear')\n",
    "        elif pd.isna(before_val) and pd.notna(after_val):\n",
    "            print(f\"âš ï¸ Gap at the start of the dataset. Forward filling for {station}.\")\n",
    "            data.loc[mask, 'Flow rate (m3/s)'] = data['Flow rate (m3/s)'].bfill()\n",
    "        elif pd.notna(before_val) and pd.isna(after_val):\n",
    "            print(f\"âš ï¸ Gap at the end of the dataset. Backward filling for {station}.\")\n",
    "            data.loc[mask, 'Flow rate (m3/s)'] = data['Flow rate (m3/s)'].ffill()\n",
    "        else:\n",
    "            print(f\"âš ï¸ Skipping gap due to insufficient data for interpolation at Station {station}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# Display stations with remaining NaNs\n",
    "def display_stations_with_remaining_nans(data):\n",
    "    stations_with_nans = []\n",
    "    for station in data['Station No.'].unique():\n",
    "        station_data = data[data['Station No.'] == station]\n",
    "        remaining_nans = identify_missing_periods(station_data, station)\n",
    "        if remaining_nans:\n",
    "            stations_with_nans.append(station)\n",
    "            print(f\"\\nâš ï¸ Station with Remaining NaNs: {station}\")\n",
    "            for start, end, days in remaining_nans:\n",
    "                print(f\"  Remaining NaN period: {start.date()} to {end.date()}, Days missing: {days}\")\n",
    "\n",
    "    if not stations_with_nans:\n",
    "        print(\"\\nâœ… No stations have remaining NaN values after interpolation.\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸ Total stations with remaining NaN values: {len(stations_with_nans)}\")\n",
    "\n",
    "# Main interpolation function by period\n",
    "def run_interpolation(WEF_Model_streamflow_data, start_date_threshold, end_date_threshold, period_name):\n",
    "    print(f\"\\nðŸ”§ Running interpolation for {period_name} period: {start_date_threshold.date()} to {end_date_threshold.date()}\")\n",
    "    for station in tqdm(WEF_Model_streamflow_data['Station No.'].unique(), desc=f\"Interpolating {period_name}\"):\n",
    "        full_station_data = WEF_Model_streamflow_data[WEF_Model_streamflow_data['Station No.'] == station]\n",
    "        target_slice = full_station_data[(full_station_data['Date'] >= start_date_threshold) & (full_station_data['Date'] <= end_date_threshold)]\n",
    "\n",
    "        if target_slice.empty:\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nâ›ï¸ Station {station} - Initial missing periods:\")\n",
    "        for start, end, days in identify_missing_periods(target_slice, station):\n",
    "            print(f\"  {start.date()} to {end.date()}, {days} days\")\n",
    "\n",
    "        interpolated_slice = interpolate_small_gaps(target_slice, station)\n",
    "\n",
    "        # Replace only the slice being processed\n",
    "        mask_to_remove = (\n",
    "            (WEF_Model_streamflow_data['Station No.'] == station) &\n",
    "            (WEF_Model_streamflow_data['Date'] >= start_date_threshold) &\n",
    "            (WEF_Model_streamflow_data['Date'] <= end_date_threshold)\n",
    "        )\n",
    "        WEF_Model_streamflow_data = WEF_Model_streamflow_data[~mask_to_remove]\n",
    "        WEF_Model_streamflow_data = pd.concat([WEF_Model_streamflow_data, interpolated_slice])\n",
    "\n",
    "    # Re-sort after all processing\n",
    "    WEF_Model_streamflow_data = WEF_Model_streamflow_data.sort_values(['Station No.', 'Date']).reset_index(drop=True)\n",
    "\n",
    "    print(f\"\\nâœ… Interpolation complete for {period_name} period.\")\n",
    "    display_stations_with_remaining_nans(WEF_Model_streamflow_data)\n",
    "    return WEF_Model_streamflow_data\n",
    "\n",
    "# === RUN INTERPOLATION FOR TRAINING PERIOD ===\n",
    "WEF_Model_streamflow_data = run_interpolation(\n",
    "    WEF_Model_streamflow_data,\n",
    "    pd.Timestamp(\"2007-01-01\"),\n",
    "    pd.Timestamp(\"2017-09-30\"),\n",
    "    \"training\"\n",
    ")\n",
    "\n",
    "# === RUN INTERPOLATION FOR TESTING PERIOD ===\n",
    "WEF_Model_streamflow_data = run_interpolation(\n",
    "    WEF_Model_streamflow_data,\n",
    "    pd.Timestamp(\"2017-10-01\"),\n",
    "    pd.Timestamp(\"2020-09-30\"),\n",
    "    \"testing\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876743ff-b9d4-495a-97b0-89c711e35b0e",
   "metadata": {
    "id": "876743ff-b9d4-495a-97b0-89c711e35b0e"
   },
   "outputs": [],
   "source": [
    "# Identify stations with remaining NaNs after interpolation\n",
    "stations_with_nans = WEF_Model_streamflow_data[WEF_Model_streamflow_data['Flow rate (m3/s)'].isna()]['Station No.'].unique()\n",
    "\n",
    "print(len(stations_with_nans))\n",
    "print(stations_with_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f0bc01-c97b-4bcc-b2e1-f5f0b04d7af5",
   "metadata": {
    "id": "68f0bc01-c97b-4bcc-b2e1-f5f0b04d7af5"
   },
   "outputs": [],
   "source": [
    "# Get unique station numbers\n",
    "stations = WEF_Model_streamflow_data['Station No.'].unique()\n",
    "print(f'No. of Stations:')\n",
    "print(len(stations))\n",
    "print(stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5265d4-51d4-4006-ab0a-6369fd6b2c43",
   "metadata": {
    "id": "5f5265d4-51d4-4006-ab0a-6369fd6b2c43"
   },
   "outputs": [],
   "source": [
    "len(WEF_Model_streamflow_data['Station No.'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2980cfac-5daf-49df-bccf-903ca355d7f1",
   "metadata": {
    "id": "2980cfac-5daf-49df-bccf-903ca355d7f1"
   },
   "outputs": [],
   "source": [
    "len(WEF_Model_Climate_data['Station No.'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0a8796-bbc8-4f5f-99e7-14465216424f",
   "metadata": {
    "id": "6d0a8796-bbc8-4f5f-99e7-14465216424f"
   },
   "outputs": [],
   "source": [
    "# Stations without downloaded climate data\n",
    "\n",
    "streamflow_only_stations = np.setdiff1d(\n",
    "    WEF_Model_streamflow_data['Station No.'].unique(),\n",
    "    WEF_Model_Climate_data['Station No.'].unique()\n",
    ")\n",
    "\n",
    "print(streamflow_only_stations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8835b34d-d623-4b21-90c1-0487b9706fd5",
   "metadata": {
    "id": "8835b34d-d623-4b21-90c1-0487b9706fd5"
   },
   "outputs": [],
   "source": [
    "# Checking the data type of the column 'Flow rate (m3/s)'\n",
    "streamflow_dtype = WEF_Model_streamflow_data['Flow rate (m3/s)'].dtype\n",
    "\n",
    "# Displaying the data type\n",
    "streamflow_dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead828cb-122c-453d-a316-90e375878683",
   "metadata": {
    "id": "ead828cb-122c-453d-a316-90e375878683"
   },
   "outputs": [],
   "source": [
    "# Identify non-numeric entries\n",
    "non_numeric_entries = WEF_Model_streamflow_data[pd.to_numeric(WEF_Model_streamflow_data['Flow rate (m3/s)'], errors='coerce').isna()]\n",
    "print(non_numeric_entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8812dc9e-d722-4ecf-9fe2-eb8492de56af",
   "metadata": {
    "id": "8812dc9e-d722-4ecf-9fe2-eb8492de56af"
   },
   "outputs": [],
   "source": [
    "WEF_Model_streamflow_data['Flow rate (m3/s)'] = pd.to_numeric(WEF_Model_streamflow_data['Flow rate (m3/s)'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc00568",
   "metadata": {
    "id": "0cc00568"
   },
   "source": [
    "# Fetching S/E Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338bd71b-7380-404b-80b2-1c153e8d9e6f",
   "metadata": {
    "id": "338bd71b-7380-404b-80b2-1c153e8d9e6f"
   },
   "source": [
    "### General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce62e874",
   "metadata": {
    "id": "ce62e874"
   },
   "outputs": [],
   "source": [
    "regions_codes = {\n",
    "     'Edirne': 'TR212',\n",
    "     'KÄ±rklareli': 'TR213',\n",
    "     'TekirdaÄŸ': 'TR211',\n",
    "     'Ä°stanbul': 'TR100',\n",
    "     'Kocaeli': 'TR421',\n",
    "     'Ã‡anakkale': 'TR222',\n",
    "     'Bursa': 'TR411',\n",
    "     'Yalova': 'TR425',\n",
    "     'BalÄ±kesir': 'TR221',\n",
    "     'KÃ¼tahya': 'TR333',\n",
    "     'Unknown': 'Unknown',\n",
    "     'Bilecik': 'TR413',\n",
    "     'Ä°zmir': 'TR310',\n",
    "     'Manisa': 'TR331',\n",
    "     'UÅŸak': 'TR334',\n",
    "     'AydÄ±n': 'TR321',\n",
    "     'Denizli': 'TR322',\n",
    "     'Afyonkarahisar': 'TR332',\n",
    "     'MuÄŸla': 'TR323',\n",
    "     'Burdur': 'TR613',\n",
    "     'Antalya': 'TR611',\n",
    "     'Isparta': 'TR612',\n",
    "     'Konya': 'TR521',\n",
    "     'Ankara': 'TR510',\n",
    "     'EskiÅŸehir': 'TR412',\n",
    "     'Sakarya': 'TR422',\n",
    "     'Bolu': 'TR424',\n",
    "     'DÃ¼zce': 'TR423',\n",
    "     'BartÄ±n': 'TR813',\n",
    "     'Kastamonu': 'TR821',\n",
    "     'Ã‡ankÄ±rÄ±': 'TR822',\n",
    "     'KarabÃ¼k': 'TR812',\n",
    "     'Zonguldak': 'TR811',\n",
    "     'Sinop': 'TR823',\n",
    "     'Samsun': 'TR831',\n",
    "     'Tunceli': 'TRB14',\n",
    "     'Amasya': 'TR834',\n",
    "     'GÃ¼mÃ¼ÅŸhane': 'TR906',\n",
    "     'Ã‡orum': 'TR833',\n",
    "     'Tokat': 'TR832',\n",
    "     'Yozgat': 'TR723',\n",
    "     'Ordu': 'TR902',\n",
    "     'Erzincan': 'TRA12',\n",
    "     'Sivas': 'TR722',\n",
    "     'Giresun': 'TR903',\n",
    "     'NiÄŸde': 'TR713',\n",
    "     'Kayseri': 'TR721',\n",
    "     'KÄ±rÄ±kkale': 'TR711',\n",
    "     'NevÅŸehir': 'TR714',\n",
    "     'KÄ±rÅŸehir': 'TR715',\n",
    "     'Aksaray': 'TR712',\n",
    "     'Karaman': 'TR522',\n",
    "     'Mersin': 'TR622',\n",
    "     'Adana': 'TR621',\n",
    "     'Kilis': 'TRC13',\n",
    "     'Gaziantep': 'TRC11',\n",
    "     'Hatay': 'TR631',\n",
    "     'KahramanmaraÅŸ': 'TR632',\n",
    "     'Osmaniye': 'TR633',\n",
    "     'AdÄ±yaman': 'TRC12',\n",
    "     'Erzurum': 'TRA11',\n",
    "     'AÄŸrÄ±': 'TRA21',\n",
    "     'BingÃ¶l': 'TRB13',\n",
    "     'Malatya': 'TRB11',\n",
    "     'ElazÄ±ÄŸ': 'TRB12',\n",
    "     'ÅžanlÄ±urfa': 'TRC21',\n",
    "     'MuÅŸ': 'TRB22',\n",
    "     'DiyarbakÄ±r': 'TRC22',\n",
    "     'Mardin': 'TRC31',\n",
    "     'Rize': 'TR904',\n",
    "     'Trabzon': 'TR901',\n",
    "     'Artvin': 'TR905',\n",
    "     'Bayburt': 'TRA13',\n",
    "     'Kars': 'TRA22',\n",
    "     'Ardahan': 'TRA24',\n",
    "     'IÄŸdÄ±r': 'TRA23',\n",
    "     'Van': 'TRB21',\n",
    "     'Bitlis': 'TRB23',\n",
    "     'Hakkari': 'TRB24',\n",
    "     'ÅžÄ±rnak': 'TRC33',\n",
    "     'Siirt': 'TRC34',\n",
    "     'Batman': 'TRC32'\n",
    "}\n",
    "\n",
    "# Invert the regions_codes dictionary to map NUTS_ID to City names\n",
    "inverted_regions_codes = {v: k for k, v in regions_codes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41332396",
   "metadata": {
    "id": "41332396"
   },
   "outputs": [],
   "source": [
    "SE_codes = {\n",
    "    \"SE_01\": \"SE_01: Average size of households\",\n",
    "    \"SE_02\": \"SE_02: Number of households by type Total\",\n",
    "    \"SE_03\": \"SE_03: Number of districts\",\n",
    "    \"SE_04\": \"SE_04: Number of municipalities\",\n",
    "    \"SE_05\": \"SE_05: Number of villages\",\n",
    "    \"SE_06\": \"SE_06: Total population\",\n",
    "    \"SE_07\": \"SE_07: Population density (capita/square km)\",\n",
    "    \"SE_08\": \"SE_08: Province and district centers population\",\n",
    "    \"SE_09\": \"SE_09: Towns and villages population\",\n",
    "\n",
    "    \"SE_10\": \"SE_10: Agricultural equipment and machinery Water pump\",\n",
    "    \"SE_11\": \"SE_11: Animal products milk (tons)\",\n",
    "    \"SE_12\": \"SE_12: Total areas for land under protective cover by type (decare)\",\n",
    "    \"SE_13\": \"SE_13: Total harvested area of cereals and other crops (hectares)\",\n",
    "    \"SE_14\": \"SE_14: Total arable land (hectare)\",\n",
    "    \"SE_15\": \"SE_15: Total land under permanent crops (hectare)\",\n",
    "    \"SE_16\": \"SE_16: Livestock Poultries (head)\",\n",
    "    \"SE_17\": \"SE_17: Livestock (head)\",\n",
    "    \"SE_18\": \"SE_18: Organic crop production area (transition period included) (Hectare)\",\n",
    "    \"SE_19\": \"SE_19: Organic crop production amount (transition period included) (Tons)\",\n",
    "    \"SE_20\": \"SE_20: Total production of area of cereals and other crops (tons)\",\n",
    "    \"SE_21\": \"SE_21: Value of animal products (1000 TL)\",\n",
    "    \"SE_22\": \"SE_22: Value of crop production (1000 TL)\",\n",
    "    \"SE_23\": \"SE_23: Value of livestock (1000 TL)\",\n",
    "    \"SE_24\": \"SE_24: Electricity consumption by Irrigation (MWh)\",\n",
    "    \"SE_25\": \"SE_25: GDP by Agriculture (1000 TRY)\",\n",
    "    \"SE_26\": \"SE_26: Number of Agriculture, forestry and fishery enterprises\",\n",
    "    \"SE_27\": \"SE_27: Exports value by Agriculture, forestry and fishing (1000 USD)\",\n",
    "    \"SE_28\": \"SE_28: Imports value by Agriculture, forestry and fishing (1000 USD)\",\n",
    "\n",
    "    \"SE_29\": \"SE_29: Fresh ground water abstraction for municipal water supply network (1000 cubic meters/year)\",\n",
    "    \"SE_30\": \"SE_30: Fresh surface water abstraction for municipal water supply network (1000 cubic meters/years)\",\n",
    "    \"SE_31\": \"SE_31: Population served by drinking water treatment plants in total municipal population\",\n",
    "    \"SE_32\": \"SE_32: Population served by water supply network in total municipal population\",\n",
    "    \"SE_33\": \"SE_33: Total investments by municipalities in water supply facilities (TL)\",\n",
    "    \"SE_34\": \"SE_34: Water abstraction per capita in municipalities (liters/capita-day)\",\n",
    "    \"SE_35\": \"SE_35: Water treated by municipalities (1000 cubic meters/year)\",\n",
    "    \"SE_36\": \"SE_36: Number of Water supply, sewerage, waste management and treatment activities enterprises\",\n",
    "    \"SE_37\": \"SE_37: Exports value by Water supply; sewerage waste management and remediation activities (1000 USD)\",\n",
    "    \"SE_38\": \"SE_38: Imports value by Water supply; sewerage waste management and remediation activities (1000 USD)\",\n",
    "\n",
    "    \"SE_39\": \"SE_39: Electricity consumption of dwellings per capita (KWh)\",\n",
    "    \"SE_40\": \"SE_40: Electricity consumption of industrial establishments per capita (KWh)\",\n",
    "    \"SE_41\": \"SE_41: Total electricity consumptions (MWh)\",\n",
    "    \"SE_42\": \"SE_42: Total electricity consumption per capita (KWh)\",\n",
    "    \"SE_43\": \"SE_43: Gross Domestic Product (1000 TRY)\",\n",
    "    \"SE_44\": \"SE_44: GDP by Industry (1000 TRY)\",\n",
    "    \"SE_45\": \"SE_45: Per capita GDP ($)\",\n",
    "    \"SE_46\": \"SE_46: Number of Manufacturing enterprises\",\n",
    "    \"SE_47\": \"SE_47: Number of Mining and quarrying enterprises\",\n",
    "    \"SE_48\": \"SE_48: Number of Accommodation and food service activities enterprises\",\n",
    "    \"SE_49\": \"SE_49: Number of Construction enterprises\",\n",
    "    \"SE_50\": \"SE_50: Number of Culture, arts, entertainment, recreation and sports enterprises\",\n",
    "    \"SE_51\": \"SE_51: Number of Eklekrtik, gas, steam and air conditioning production and distribution enterprises\",\n",
    "    \"SE_52\": \"SE_52: Total number of enterprises\",\n",
    "    \"SE_53\": \"SE_53: Exports value by Electricity, gas, steam and air conditioning supply (1000 USD)\",\n",
    "    \"SE_54\": \"SE_54: General total exports value (1000 USD)\",\n",
    "    \"SE_55\": \"SE_55: Exports value by Manufacturing (1000 USD)\",\n",
    "    \"SE_56\": \"SE_56: Exports value by Mining and quarrying (1000 USD)\",\n",
    "    \"SE_57\": \"SE_57: Imports value by Electricity, gas, steam and air conditioning supply (1000 USD)\",\n",
    "    \"SE_58\": \"SE_58: Imports value by General total (1000 USD)\",\n",
    "    \"SE_59\": \"SE_59: Imports value by Manufacturing (1000 USD)\",\n",
    "    \"SE_60\": \"SE_60: Imports value by Mining and quarrying (1000 USD)\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559cea9",
   "metadata": {
    "id": "9559cea9"
   },
   "outputs": [],
   "source": [
    "SE_data = pd.read_csv('/***/***/SE_data.csv') # Replace with the actual path to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f5c45-c63a-4a79-ba3d-50eceeebbcd5",
   "metadata": {
    "id": "723f5c45-c63a-4a79-ba3d-50eceeebbcd5"
   },
   "outputs": [],
   "source": [
    "SE_data = SE_data.sort_values(by=[\"NUTS_ID\", \"Year\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a9c041e",
   "metadata": {
    "id": "8a9c041e"
   },
   "outputs": [],
   "source": [
    "SE_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ad9b47-18b5-480f-8a95-75275d148dd2",
   "metadata": {
    "id": "72ad9b47-18b5-480f-8a95-75275d148dd2"
   },
   "outputs": [],
   "source": [
    "SE_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5814605",
   "metadata": {
    "id": "a5814605"
   },
   "outputs": [],
   "source": [
    "intervals = {}\n",
    "\n",
    "# Loop through each unique \"NUTS_ID\" and each \"SE\" column to identify the intervals and missing years\n",
    "for nuts_id in SE_data['NUTS_ID'].unique():\n",
    "    intervals[nuts_id] = {}\n",
    "    for column in SE_data.columns:\n",
    "        if column.startswith('SE'):\n",
    "            # Filter data for the current NUTS_ID and column\n",
    "            non_null_years = SE_data[(SE_data['NUTS_ID'] == nuts_id) & SE_data[column].notnull()]['Year']\n",
    "\n",
    "            # Check if there are any non-null years\n",
    "            if not non_null_years.empty:\n",
    "                start_year = non_null_years.min()\n",
    "                end_year = non_null_years.max()\n",
    "                full_years = set(range(start_year, end_year + 1))\n",
    "                available_years = set(non_null_years)\n",
    "                missing_years = sorted(full_years - available_years)\n",
    "\n",
    "                # Store the results for each NUTS_ID and SE column\n",
    "                intervals[nuts_id][column] = {\n",
    "                    'Start Year': start_year,\n",
    "                    'End Year': end_year,\n",
    "                    'Missing Years': missing_years\n",
    "                }\n",
    "\n",
    "# Convert the dictionary into a DataFrame with MultiIndex for better readability\n",
    "intervals_df = pd.concat({k: pd.DataFrame(v).T for k, v in intervals.items()}, axis=0)\n",
    "intervals_df.index.names = ['NUTS_ID', 'SE']\n",
    "intervals_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2924f54f",
   "metadata": {
    "id": "2924f54f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Print the DataFrame\n",
    "print(intervals_df)\n",
    "# Optionally, reset to default after printing (if you don't want this setting to affect future outputs)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9fcafc-da92-4d7e-911b-aa870466e6e9",
   "metadata": {
    "id": "4b9fcafc-da92-4d7e-911b-aa870466e6e9"
   },
   "outputs": [],
   "source": [
    "# Print the DataFrame\n",
    "intervals_df"
   ]
  },
  {
   "cell_type": "raw",
   "id": "990d76f0-9c64-4fba-83fc-d85e1c629bee",
   "metadata": {
    "id": "990d76f0-9c64-4fba-83fc-d85e1c629bee"
   },
   "source": [
    "SE06: Total population\n",
    "SE07: Population density (capita/square km)\n",
    "SE08: Province and district centers population\n",
    "SE10: Agricultural equipment and machinery Water pump\n",
    "SE13: Total harvested area of cereals and other crops (hectares)\n",
    "SE14: Total arable land (hectare)\n",
    "SE15: Total land under permanent crops (hectare)\n",
    "SE17: Livestock (head)\n",
    "SE29: Fresh ground water abstraction for municipal water supply network (1000 cubic meters/year)\n",
    "SE32: Population served by water supply network in total municipal population\n",
    "SE34: Water abstraction per capita in municipalities (liters/capita-day)\n",
    "SE39: Electricity consumption of dwellings per capita (KWh)\n",
    "SE40: Electricity consumption of industrial establishments per capita (KWh)\n",
    "SE45: Per capita GDP ($)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ef5d9b-9353-4ca7-ae01-a3332fda1240",
   "metadata": {
    "id": "71ef5d9b-9353-4ca7-ae01-a3332fda1240"
   },
   "outputs": [],
   "source": [
    "# Filter only the parameters you're interested in\n",
    "selected_SEs = [\n",
    "    \"SE06\", \"SE07\", \"SE08\", \"SE10\", \"SE13\", \"SE14\", \"SE15\",\n",
    "    \"SE17\", \"SE29\", \"SE32\", \"SE34\", \"SE39\", \"SE40\", \"SE45\"\n",
    "]\n",
    "\n",
    "filtered_df = intervals_df[intervals_df[\"SE\"].isin(selected_SEs)]\n",
    "\n",
    "# Group by SE and Start Year, then aggregate the NUTS_IDs under each\n",
    "selected_SEs_av = (\n",
    "    filtered_df\n",
    "    .groupby([\"SE\", \"Start Year\"])[\"NUTS_ID\"]\n",
    "    .apply(list)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Rename columns for clarity\n",
    "selected_SEs_av.columns = [\"SE\", \"Start Year\", \"NUTS_IDs\"]\n",
    "selected_SEs_av"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3eb6142-5355-4777-bcac-315bba216e93",
   "metadata": {
    "id": "f3eb6142-5355-4777-bcac-315bba216e93"
   },
   "source": [
    "### Interpolating missing S/E values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68218d1",
   "metadata": {
    "id": "c68218d1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List to store columns with missing years\n",
    "SE_Missing_Years = []\n",
    "\n",
    "# Loop through each row in intervals_df\n",
    "for index, row in intervals_df.iterrows():\n",
    "    # Check if the SE column name starts with \"SE\"\n",
    "    if row['SE'].startswith('SE'):\n",
    "        # Check if there are any missing years\n",
    "        if row[\"Missing Years\"]:\n",
    "            SE_Missing_Years.append(row['SE'])\n",
    "            print(f\"Missing years in {row['SE']} for NUTS_ID {row['NUTS_ID']}: {row['Missing Years']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ecd84b-c477-4dd8-8693-50066a4d5978",
   "metadata": {
    "id": "56ecd84b-c477-4dd8-8693-50066a4d5978",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for nuts_id in SE_data['NUTS_ID'].unique():\n",
    "    # Filter SE_data for the current NUTS_ID\n",
    "    nuts_data = SE_data[SE_data['NUTS_ID'] == nuts_id]\n",
    "\n",
    "    for column in SE_Missing_Years:\n",
    "        # Ensure the column exists in SE_data\n",
    "        if column in nuts_data.columns:\n",
    "            # Retrieve the missing years information for the current NUTS_ID and column\n",
    "            missing_years = intervals_df[(intervals_df['NUTS_ID'] == nuts_id) & (intervals_df['SE'] == column)][\"Missing Years\"].values\n",
    "\n",
    "            if missing_years.size > 0:\n",
    "                # print(f\"Missing years in {column} for NUTS_ID {nuts_id}:\", missing_years[0])\n",
    "\n",
    "                # Create masks to check for valid data before and after each NaN\n",
    "                valid_mask_before = nuts_data[column].shift(1).notna()\n",
    "                valid_mask_after = nuts_data[column].shift(-1).notna()\n",
    "\n",
    "                # Interpolate the data\n",
    "                interpolated = nuts_data[column].interpolate(method='linear', limit_direction='both')\n",
    "\n",
    "                # Update only the NaN values with valid neighbors\n",
    "                SE_data.loc[nuts_data.index, column] = nuts_data[column].where(\n",
    "                    nuts_data[column].notna() | ~(valid_mask_before & valid_mask_after),\n",
    "                    interpolated\n",
    "                )\n",
    "\n",
    "                # Display the interpolated data for this NUTS_ID and column\n",
    "                # print(SE_data[SE_data['NUTS_ID'] == nuts_id][[\"Year\", \"NUTS_ID\", column]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26125185-57aa-48a7-b67c-1f25b7c69d3b",
   "metadata": {
    "id": "26125185-57aa-48a7-b67c-1f25b7c69d3b"
   },
   "outputs": [],
   "source": [
    "SE_data.loc[:, 'SE_31'] = (SE_data['SE_31'] / 100) * SE_data['SE_06']\n",
    "SE_data.loc[:, 'SE_32'] = (SE_data['SE_32'] / 100) * SE_data['SE_06']\n",
    "SE_data.loc[:, 'SE_34'] = (SE_data['SE_34'] / 100) * SE_data['SE_06']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f702d-b146-4dd9-baa7-e5fc95487272",
   "metadata": {
    "id": "ac8f702d-b146-4dd9-baa7-e5fc95487272"
   },
   "outputs": [],
   "source": [
    "intervals = {}\n",
    "\n",
    "# Loop through each unique \"NUTS_ID\" and each \"SE\" column to identify the intervals and missing years\n",
    "for nuts_id in SE_data['NUTS_ID'].unique():\n",
    "    intervals[nuts_id] = {}\n",
    "    for column in SE_data.columns:\n",
    "        if column.startswith('SE'):\n",
    "            # Filter data for the current NUTS_ID and column\n",
    "            non_null_years = SE_data[(SE_data['NUTS_ID'] == nuts_id) & SE_data[column].notnull()]['Year']\n",
    "\n",
    "            # Check if there are any non-null years\n",
    "            if not non_null_years.empty:\n",
    "                start_year = non_null_years.min()\n",
    "                end_year = non_null_years.max()\n",
    "                full_years = set(range(start_year, end_year + 1))\n",
    "                available_years = set(non_null_years)\n",
    "                missing_years = sorted(full_years - available_years)\n",
    "\n",
    "                # Store the results for each NUTS_ID and SE column\n",
    "                intervals[nuts_id][column] = {\n",
    "                    'Start Year': start_year,\n",
    "                    'End Year': end_year,\n",
    "                    'Missing Years': missing_years\n",
    "                }\n",
    "\n",
    "# Convert the dictionary into a DataFrame with MultiIndex for better readability\n",
    "intervals_df = pd.concat({k: pd.DataFrame(v).T for k, v in intervals.items()}, axis=0)\n",
    "intervals_df.index.names = ['NUTS_ID', 'SE']\n",
    "intervals_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732cbaa6-29ae-4721-b7b9-e82736bd9c75",
   "metadata": {
    "id": "732cbaa6-29ae-4721-b7b9-e82736bd9c75",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "# Print the DataFrame\n",
    "print(intervals_df)\n",
    "# Optionally, reset to default after printing (if you don't want this setting to affect future outputs)\n",
    "pd.reset_option('display.max_rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0112cb3f-a0a1-438b-a7f1-2cf72d3346ac",
   "metadata": {
    "id": "0112cb3f-a0a1-438b-a7f1-2cf72d3346ac"
   },
   "outputs": [],
   "source": [
    "# Print the DataFrame\n",
    "intervals_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcb47ab-ec8e-42f6-80e0-c6569faba14f",
   "metadata": {
    "id": "bbcb47ab-ec8e-42f6-80e0-c6569faba14f"
   },
   "source": [
    "### Eliminating cities outside Model area of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3e360c-8b5a-423c-9b82-97177207a155",
   "metadata": {
    "id": "cd3e360c-8b5a-423c-9b82-97177207a155"
   },
   "outputs": [],
   "source": [
    "WEF_Model_WSHEDS_SA = pd.read_excel('/***/***/StreamflowStationsWSHEDS_SA.xlsx')# Replace with the actual path to the file\n",
    "WEF_Model_WSHEDS_SA = WEF_Model_WSHEDS_SA.drop(\"Station No.\", axis=1)\n",
    "\n",
    "WEF_Model_NUTS_IDs = set()\n",
    "\n",
    "# Iterate over each row in WEF_Model_WSHEDS_SA (each station)\n",
    "for index, station_row in WEF_Model_WSHEDS_SA.iterrows():\n",
    "    # Filter columns in station_row where the value is greater than 0.10 to get the cities and their weights\n",
    "    city_weights = station_row[station_row > 0.10].drop('Station No.', errors='ignore')\n",
    "\n",
    "    # Add unique cities (NUTS_IDs) to the set\n",
    "    WEF_Model_NUTS_IDs.update(city_weights.index)\n",
    "\n",
    "# Convert the set of unique NUTS_IDs to a list\n",
    "WEF_Model_NUTS_IDs = list(WEF_Model_NUTS_IDs)\n",
    "print(\"Unique NUTS_IDs (cities) with weights > 0.10:\", WEF_Model_NUTS_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0425f18b",
   "metadata": {
    "id": "0425f18b"
   },
   "outputs": [],
   "source": [
    "SE_data_ = SE_data[SE_data['NUTS_ID'].isin(WEF_Model_NUTS_IDs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f91ddf6-42d0-453f-a2c8-79b9754ac934",
   "metadata": {
    "id": "3f91ddf6-42d0-453f-a2c8-79b9754ac934"
   },
   "outputs": [],
   "source": [
    "SE_data_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb256fc-fc1f-49ea-8c89-15da8e765b68",
   "metadata": {
    "id": "2eb256fc-fc1f-49ea-8c89-15da8e765b68"
   },
   "source": [
    "### Eliminating S/E indicators with missing data in the period of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26b92ab",
   "metadata": {
    "id": "b26b92ab"
   },
   "outputs": [],
   "source": [
    "SE_Start_Year = 2007\n",
    "SE_End_Year = 2021\n",
    "\n",
    "# Dictionary to store filtered SE_data for each NUTS_ID\n",
    "filtered_SE_data_by_nuts_id = {}\n",
    "\n",
    "# Initialize a set to track the columns that exist in every city and start with 'SE'\n",
    "WEF_Model_SE_IDs = None\n",
    "\n",
    "for nuts_id in SE_data_['NUTS_ID'].unique():\n",
    "    # Collect `SE` columns that meet the criteria for this specific NUTS_ID\n",
    "    q_parameters = [\n",
    "        param for param, details in intervals.get(nuts_id, {}).items()\n",
    "        if details['Start Year'] <= SE_Start_Year and\n",
    "           details['End Year'] >= SE_End_Year and\n",
    "           not details['Missing Years']\n",
    "    ]\n",
    "\n",
    "    # Define the columns to keep, including the required Year and NUTS_ID columns\n",
    "    columns_to_keep = ['Year', 'NUTS_ID'] + q_parameters\n",
    "\n",
    "    # Filter SE_data for this specific NUTS_ID and include only the specified columns\n",
    "    nuts_data = SE_data_[SE_data_['NUTS_ID'] == nuts_id][columns_to_keep]\n",
    "\n",
    "    # Further filter for the specified year range\n",
    "    nuts_data = nuts_data[(nuts_data['Year'] >= SE_Start_Year) & (nuts_data['Year'] <= SE_End_Year)]\n",
    "\n",
    "    # Store the filtered data for this NUTS_ID\n",
    "    filtered_SE_data_by_nuts_id[nuts_id] = nuts_data\n",
    "\n",
    "    # Track common columns: initialize with the first city's \"SE\" columns, then intersect with each subsequent city\n",
    "    se_columns = {col for col in nuts_data.columns if col.startswith(\"SE\")}\n",
    "    if WEF_Model_SE_IDs is None:\n",
    "        WEF_Model_SE_IDs = se_columns\n",
    "    else:\n",
    "        WEF_Model_SE_IDs.intersection_update(se_columns)\n",
    "\n",
    "WEF_Model_SE_IDs = sorted(WEF_Model_SE_IDs)\n",
    "print(\"No. of WEF_Model SE:\", len(WEF_Model_SE_IDs))\n",
    "print(\"WEF_Model_SE_IDs:\", WEF_Model_SE_IDs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e8377",
   "metadata": {
    "id": "c92e8377"
   },
   "outputs": [],
   "source": [
    "# Create a list of tuples (code, name) for each SE indicator in WEF_Model_SE_IDs\n",
    "SE_indicators_with_names = [(code, SE_codes.get(code, f\"Unknown_{code}\")) for code in WEF_Model_SE_IDs]\n",
    "\n",
    "# Display the result\n",
    "for code, name in SE_indicators_with_names:\n",
    "    print(f\"{code}: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e19e7de",
   "metadata": {
    "id": "9e19e7de"
   },
   "outputs": [],
   "source": [
    "# Filter SE_data to include only the columns in WEF_Model_SE_IDs\n",
    "SE_data__ = SE_data_[['Year', 'NUTS_ID'] + WEF_Model_SE_IDs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec00f018-ddd8-44fb-b740-e2f2cca590b5",
   "metadata": {
    "id": "ec00f018-ddd8-44fb-b740-e2f2cca590b5"
   },
   "outputs": [],
   "source": [
    "SE_data__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949bc3d4-5c6c-4761-804b-2c83189f3e38",
   "metadata": {
    "id": "949bc3d4-5c6c-4761-804b-2c83189f3e38"
   },
   "source": [
    "### Eliminating years outside Model period of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda48ac2-d1b1-4928-b71d-8edc45b87ae8",
   "metadata": {
    "id": "bda48ac2-d1b1-4928-b71d-8edc45b87ae8"
   },
   "outputs": [],
   "source": [
    "SE_Start_Year = 2007\n",
    "SE_End_Year = 2020\n",
    "\n",
    "# Filter SE_data to include only the columns in WEF_Model_SE_IDs and keep only rows within the specified year range\n",
    "SE_data___ = SE_data__[(SE_data__['Year'] >= SE_Start_Year) & (SE_data__['Year'] <= SE_End_Year)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7563ea-5078-4e4d-a49d-867d16bacba9",
   "metadata": {
    "id": "ed7563ea-5078-4e4d-a49d-867d16bacba9"
   },
   "outputs": [],
   "source": [
    "SE_data___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec856639-e7e1-4bbb-9446-9c0581771d9e",
   "metadata": {
    "id": "ec856639-e7e1-4bbb-9446-9c0581771d9e"
   },
   "outputs": [],
   "source": [
    "SE_data = SE_data___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bebb75ad-fe6f-488a-8d46-e02b1bb2acf7",
   "metadata": {
    "id": "bebb75ad-fe6f-488a-8d46-e02b1bb2acf7"
   },
   "source": [
    "### Selected SEs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ab38ad-070b-45b5-bdfa-61062755be80",
   "metadata": {
    "id": "99ab38ad-070b-45b5-bdfa-61062755be80"
   },
   "outputs": [],
   "source": [
    "selected_SEs = [\n",
    "    \"Year\", \"NUTS_ID\",\n",
    "    \"SE_06\", \"SE_07\", \"SE_08\", \"SE_10\", \"SE_13\", \"SE_14\", \"SE_15\",\n",
    "    \"SE_17\", \"SE_29\", \"SE_32\", \"SE_34\", \"SE_39\", \"SE_40\", \"SE_45\"\n",
    "]\n",
    "\n",
    "SE_data = SE_data[selected_SEs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2053a882-a187-4352-9c2d-ea06b6fea1f0",
   "metadata": {
    "id": "2053a882-a187-4352-9c2d-ea06b6fea1f0"
   },
   "outputs": [],
   "source": [
    "SE_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0744ed6-5f07-403f-95e5-a15c113e1d16",
   "metadata": {
    "id": "b0744ed6-5f07-403f-95e5-a15c113e1d16"
   },
   "source": [
    "### Monthly & Daily S/E values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ed926",
   "metadata": {
    "id": "df0ed926"
   },
   "outputs": [],
   "source": [
    "# Fixed Monthly proportions\n",
    "monthly_proportions = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Function to disaggregate yearly values\n",
    "def disaggregate_yearly_to_monthly(yearly_value, proportions):\n",
    "    return [yearly_value * proportion for proportion in proportions]\n",
    "\n",
    "# Apply the function to each relevant column\n",
    "monthly_data = {}\n",
    "for column in SE_data.columns:\n",
    "    if column.startswith('SE'):\n",
    "        yearly_values = SE_data[column].values\n",
    "        monthly_values = [disaggregate_yearly_to_monthly(yearly_value, monthly_proportions) for yearly_value in yearly_values]\n",
    "        monthly_values_flat = [value for sublist in monthly_values for value in sublist]\n",
    "        monthly_data[column] = monthly_values_flat\n",
    "\n",
    "# Create a new DataFrame for monthly data\n",
    "years = SE_data['Year'].values\n",
    "NUTS_IDs = SE_data['NUTS_ID'].values  # Extract NUTS_ID values\n",
    "months = list(range(1, 13))\n",
    "\n",
    "# Expand the Year and NUTS_ID columns for monthly data\n",
    "monthly_years = [year for year in years for _ in months]\n",
    "monthly_months = months * len(years)\n",
    "monthly_nuts_ids = [nuts_id for nuts_id in NUTS_IDs for _ in months]  # Repeat NUTS_ID for each month\n",
    "\n",
    "# Create the DataFrame\n",
    "SE_monthly_df = pd.DataFrame(monthly_data)\n",
    "SE_monthly_df.insert(0, 'NUTS_ID', monthly_nuts_ids)\n",
    "SE_monthly_df.insert(1, 'Year', monthly_years)\n",
    "SE_monthly_df.insert(2, 'Month', monthly_months)\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "# import ace_tools as tools; tools.display_dataframe_to_user(name=\"Disaggregated Monthly Data with NUTS_ID\", dataframe=SE_monthly_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103bc8b6",
   "metadata": {
    "id": "103bc8b6"
   },
   "outputs": [],
   "source": [
    "SE_monthly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d45a4cb5-6e10-48d9-8922-8285927c67d8",
   "metadata": {
    "id": "d45a4cb5-6e10-48d9-8922-8285927c67d8"
   },
   "outputs": [],
   "source": [
    "# SE_monthly_df[SE_monthly_df[\"NUTS_ID\"] == \"TRC34\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89975102",
   "metadata": {
    "id": "89975102"
   },
   "outputs": [],
   "source": [
    "# Initialize dictionaries and lists for daily data\n",
    "daily_data = {}\n",
    "date_list = []\n",
    "nuts_id_list = []  # List to store repeated NUTS_IDs for daily data\n",
    "\n",
    "# Generate daily data for each unique NUTS_ID and month\n",
    "for nuts_id in SE_monthly_df['NUTS_ID'].unique():\n",
    "    # Filter data for the current NUTS_ID\n",
    "    nuts_data = SE_monthly_df[SE_monthly_df['NUTS_ID'] == nuts_id]\n",
    "\n",
    "    for year, month in zip(nuts_data['Year'], nuts_data['Month']):\n",
    "        # Generate the list of dates for each month for this specific NUTS_ID\n",
    "        num_days = pd.Period(f'{year}-{month}').days_in_month\n",
    "        dates = pd.date_range(start=f'{year}-{month}-01', periods=num_days)\n",
    "\n",
    "        # Append generated dates and NUTS_IDs to lists\n",
    "        date_list.extend(dates)\n",
    "        nuts_id_list.extend([nuts_id] * num_days)\n",
    "\n",
    "        # Generate daily data for each column based on monthly values\n",
    "        for column in SE_monthly_df.columns[3:]:  # Skip 'Year', 'Month', and 'NUTS_ID'\n",
    "            if column not in daily_data:\n",
    "                daily_data[column] = []\n",
    "\n",
    "            # Get the monthly value and extend it for each day in the month\n",
    "            monthly_value = nuts_data[(nuts_data['Year'] == year) & (nuts_data['Month'] == month)][column].values[0]\n",
    "            daily_data[column].extend([monthly_value] * num_days)\n",
    "\n",
    "# Create the new DataFrame with 'Date' and 'NUTS_ID' columns\n",
    "SE_daily_df = pd.DataFrame(daily_data)\n",
    "SE_daily_df.insert(0, 'NUTS_ID', nuts_id_list)\n",
    "SE_daily_df.insert(1, 'Date', date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e211b2",
   "metadata": {
    "id": "91e211b2"
   },
   "outputs": [],
   "source": [
    "SE_daily_df = SE_daily_df.sort_values(by=[\"NUTS_ID\", \"Date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf95548",
   "metadata": {
    "id": "daf95548"
   },
   "outputs": [],
   "source": [
    "SE_daily_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd19aeb-96f4-4e42-8221-979f1228d3d2",
   "metadata": {
    "id": "0bd19aeb-96f4-4e42-8221-979f1228d3d2"
   },
   "source": [
    "### SE Indicators by Watersheds (Station No.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65b5ab7-98ce-4460-9415-fa93a157d8c8",
   "metadata": {
    "id": "d65b5ab7-98ce-4460-9415-fa93a157d8c8"
   },
   "outputs": [],
   "source": [
    "WEF_Model_WSHEDS_SA = pd.read_excel('/***/***/StreamflowStationsWSHEDS_SA.xlsx') # Replace with the actual path to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd30c0f1-b86b-475e-959e-815a71372f8b",
   "metadata": {
    "id": "fd30c0f1-b86b-475e-959e-815a71372f8b"
   },
   "outputs": [],
   "source": [
    "WEF_Model_WSHEDS_SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec00eb8-7332-486c-9fbb-0d1cfec83373",
   "metadata": {
    "id": "3ec00eb8-7332-486c-9fbb-0d1cfec83373"
   },
   "outputs": [],
   "source": [
    "WEF_Model_WSHEDS_A = pd.read_excel('/***/***/StreamflowStationsWSHEDS_SA(km2).xlsx') # Replace with the actual path to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8b84ef-5759-4b5b-bd17-1cf760970cab",
   "metadata": {
    "id": "2d8b84ef-5759-4b5b-bd17-1cf760970cab"
   },
   "outputs": [],
   "source": [
    "WEF_Model_WSHEDS_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0120b25-fac3-4343-a572-6cfeb8876538",
   "metadata": {
    "id": "e0120b25-fac3-4343-a572-6cfeb8876538"
   },
   "outputs": [],
   "source": [
    "WEF_Model_CITIES_A = pd.read_excel('/***/***/Turkish_Cities_Area(km2).xlsx') # Replace with the actual path to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5d66a2-3818-467c-8806-97d7b39a7f6e",
   "metadata": {
    "id": "4e5d66a2-3818-467c-8806-97d7b39a7f6e"
   },
   "outputs": [],
   "source": [
    "WEF_Model_CITIES_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc92b602-23b2-43e1-b92e-0bff2ef4519d",
   "metadata": {
    "id": "dc92b602-23b2-43e1-b92e-0bff2ef4519d"
   },
   "outputs": [],
   "source": [
    "# WEF_Model_WSHEDS_SA = WEF_Model_WSHEDS_SA[~WEF_Model_WSHEDS_SA['Station No.'].isin(stations_to_drop)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deba243-5561-4f9d-9ad9-533014f450a3",
   "metadata": {
    "id": "0deba243-5561-4f9d-9ad9-533014f450a3"
   },
   "outputs": [],
   "source": [
    "# Step 1: Melt city-area-in-watershed table to long format\n",
    "area_long = WEF_Model_WSHEDS_A.melt(\n",
    "    id_vars='Station No.',\n",
    "    var_name='NUTS_ID',\n",
    "    value_name='City_Area_in_Watershed'\n",
    ")\n",
    "\n",
    "# Step 2: Merge with city total area\n",
    "area_long = area_long.merge(\n",
    "    WEF_Model_CITIES_A[['NUTS_ID', 'Area_calc']],\n",
    "    on='NUTS_ID',\n",
    "    how='left'\n",
    ")\n",
    "area_long = area_long.rename(columns={'Area_calc': 'City_Total_Area'})\n",
    "\n",
    "# Step 3: Merge with watershed total area\n",
    "area_long = area_long.merge(\n",
    "    WEF_Model_info_data_07[['Station No.', 'Area_calc']],\n",
    "    on='Station No.',\n",
    "    how='left'\n",
    ")\n",
    "area_long = area_long.rename(columns={'Area_calc': 'Watershed_Total_Area'})\n",
    "\n",
    "# Step 4: Drop zero or null contributions\n",
    "area_long = area_long[area_long['City_Area_in_Watershed'] > 0].copy()\n",
    "\n",
    "# Step 5: Calculate unnormalized share metrics\n",
    "area_long['City_Share'] = area_long['City_Area_in_Watershed'] / area_long['City_Total_Area']\n",
    "area_long['Station_Share'] = area_long['City_Area_in_Watershed'] / area_long['Watershed_Total_Area']\n",
    "\n",
    "# âœ… Step 6: Filter cities that contribute at least 10% to the station\n",
    "area_long = area_long[area_long['Station_Share'] > 0.10].copy()\n",
    "\n",
    "# âœ… Step 7: Normalize Station_Share per station (so sum = 1)\n",
    "area_long['Norm_Station_Share'] = area_long.groupby('Station No.')['Station_Share'].transform(lambda x: x / x.sum())\n",
    "\n",
    "# âœ… Step 8: Final Adjusted Weight for SE aggregation\n",
    "area_long['SE_Adj_Weight'] = area_long['City_Share'] * area_long['Norm_Station_Share']\n",
    "\n",
    "# Reset index for neatness\n",
    "area_long.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dec51a-4972-47cf-bd04-72e06d2b7592",
   "metadata": {
    "id": "53dec51a-4972-47cf-bd04-72e06d2b7592"
   },
   "outputs": [],
   "source": [
    "area_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba29e03-88b2-4784-9145-a0be55edd8cd",
   "metadata": {
    "id": "aba29e03-88b2-4784-9145-a0be55edd8cd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_data = []\n",
    "\n",
    "# Loop through each station and its contributing cities\n",
    "for station, group in area_long.groupby('Station No.'):\n",
    "    weighted_se_data = pd.DataFrame()\n",
    "\n",
    "    for _, row in group.iterrows():\n",
    "        city_id = row['NUTS_ID']\n",
    "        weight = row['SE_Adj_Weight']\n",
    "\n",
    "        # Get SE data for the city\n",
    "        city_se = SE_daily_df[SE_daily_df['NUTS_ID'] == city_id].copy()\n",
    "        if city_se.empty:\n",
    "            continue  # Skip if no SE data for this city\n",
    "\n",
    "        # Multiply each SE column by the adjusted weight\n",
    "        weighted = city_se.loc[:, 'SE_06':].multiply(weight)\n",
    "\n",
    "        # Add 'Date' back\n",
    "        weighted['Date'] = city_se['Date'].values\n",
    "\n",
    "        # Accumulate weighted SEs by date\n",
    "        weighted.set_index('Date', inplace=True)\n",
    "\n",
    "        if weighted_se_data.empty:\n",
    "            weighted_se_data = weighted\n",
    "        else:\n",
    "            weighted_se_data = weighted_se_data.add(weighted, fill_value=0)\n",
    "\n",
    "    # Finalize the DataFrame for this station\n",
    "    weighted_se_data.reset_index(inplace=True)\n",
    "    weighted_se_data['Station No.'] = station\n",
    "\n",
    "    # Order columns: Station No., Date, SE values\n",
    "    ordered_cols = ['Station No.', 'Date'] + [col for col in weighted_se_data.columns if col not in ['Station No.', 'Date']]\n",
    "    weighted_se_data = weighted_se_data[ordered_cols]\n",
    "\n",
    "    result_data.append(weighted_se_data)\n",
    "\n",
    "# Combine all station-level data\n",
    "WEF_Model_SE_data = pd.concat(result_data, ignore_index=True)\n",
    "WEF_Model_SE_data.sort_values(by=['Station No.', 'Date'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00ec551-d114-44d6-981f-8919efaf4a8d",
   "metadata": {
    "id": "e00ec551-d114-44d6-981f-8919efaf4a8d"
   },
   "outputs": [],
   "source": [
    "# Select and reorder key columns for clarity\n",
    "station_city_mapping = area_long[[\n",
    "    'Station No.',\n",
    "    'NUTS_ID',\n",
    "    'City_Area_in_Watershed',\n",
    "    'City_Total_Area',\n",
    "    'Watershed_Total_Area',\n",
    "    'City_Share',\n",
    "    'Station_Share',\n",
    "    'Norm_Station_Share',\n",
    "    'SE_Adj_Weight'\n",
    "]].copy()\n",
    "\n",
    "# Optional: round values for readability\n",
    "station_city_mapping = station_city_mapping.round({\n",
    "    'City_Area_in_Watershed': 3,\n",
    "    'City_Total_Area': 3,\n",
    "    'Watershed_Total_Area': 3,\n",
    "    'City_Share': 4,\n",
    "    'Station_Share': 4,\n",
    "    'Normalized_Station_Share': 4,\n",
    "    'Adjusted_Weight': 4\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c501787-f03f-4732-abe2-016684fe44f4",
   "metadata": {
    "id": "9c501787-f03f-4732-abe2-016684fe44f4"
   },
   "outputs": [],
   "source": [
    "station_city_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7235e52-d336-4ac6-80c7-dd6d1d4db7c0",
   "metadata": {
    "id": "e7235e52-d336-4ac6-80c7-dd6d1d4db7c0"
   },
   "source": [
    "## save station_city_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234a6a9-f7f5-49fa-987b-8e66c43193b7",
   "metadata": {
    "id": "1234a6a9-f7f5-49fa-987b-8e66c43193b7"
   },
   "outputs": [],
   "source": [
    "# # Save model_histories to a file\n",
    "# with open('/***/***/station_city_mapping.pkl', 'wb') as file: # Replace with the actual path to the file\n",
    "#     pickle.dump(station_city_mapping, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca409b7-456f-448d-9f0b-b41767b73ab1",
   "metadata": {
    "id": "bca409b7-456f-448d-9f0b-b41767b73ab1"
   },
   "outputs": [],
   "source": [
    "# # Load model_histories from a file\n",
    "# with open('/***/***/station_city_mapping.pkl', 'rb') as file: # Replace with the actual path to the file\n",
    "#     station_city_mapping = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7bae11-6d3b-49c8-bc58-b331c549bdef",
   "metadata": {
    "id": "cd7bae11-6d3b-49c8-bc58-b331c549bdef"
   },
   "outputs": [],
   "source": [
    "# WEF_Model_SE_data[WEF_Model_SE_data[\"Station No.\"] == \"D07A009\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed81be",
   "metadata": {
    "id": "62ed81be"
   },
   "source": [
    "# Model Full dataset (Streamflow, S/E indicators, and Climate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa5b67a-cb9f-4906-ba56-d296a03a668f",
   "metadata": {
    "id": "3fa5b67a-cb9f-4906-ba56-d296a03a668f"
   },
   "outputs": [],
   "source": [
    "WEF_Model_streamflow_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df8b238-25c8-4d77-abc8-9471fe02dcbe",
   "metadata": {
    "id": "7df8b238-25c8-4d77-abc8-9471fe02dcbe"
   },
   "outputs": [],
   "source": [
    "WEF_Model_streamflow_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42aaf7c3-3df4-4a31-9aa7-267b39b9a335",
   "metadata": {
    "id": "42aaf7c3-3df4-4a31-9aa7-267b39b9a335"
   },
   "outputs": [],
   "source": [
    "WEF_Model_Climate_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b19e5e7-115f-4106-b45c-ddf9c927a098",
   "metadata": {
    "id": "0b19e5e7-115f-4106-b45c-ddf9c927a098"
   },
   "outputs": [],
   "source": [
    "WEF_Model_Climate_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f057bfe2-3b4e-457b-a547-45b8a7f6ad29",
   "metadata": {
    "id": "f057bfe2-3b4e-457b-a547-45b8a7f6ad29"
   },
   "outputs": [],
   "source": [
    "WEF_Model_SE_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d119e1c5-9ee6-4e9e-9901-b0b097d6a5a1",
   "metadata": {
    "id": "d119e1c5-9ee6-4e9e-9901-b0b097d6a5a1"
   },
   "outputs": [],
   "source": [
    "WEF_Model_SE_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae90bf11-f3d0-41e5-a01f-224214e4f54d",
   "metadata": {
    "id": "ae90bf11-f3d0-41e5-a01f-224214e4f54d"
   },
   "outputs": [],
   "source": [
    "WEF_Model_streamflow_data = WEF_Model_streamflow_data.copy()\n",
    "WEF_Model_Climate_data = WEF_Model_Climate_data.copy()\n",
    "WEF_Model_SE_data = WEF_Model_SE_data.copy()\n",
    "\n",
    "WEF_Model_streamflow_data['Date'] = pd.to_datetime(WEF_Model_streamflow_data['Date'])\n",
    "WEF_Model_Climate_data['Date'] = pd.to_datetime(WEF_Model_Climate_data['Date'])\n",
    "WEF_Model_SE_data['Date'] = pd.to_datetime(WEF_Model_SE_data['Date'])\n",
    "\n",
    "# Step 1: Merge WEF_Model_streamflow_data and WEF_Model_Climate_data on 'Station No.' and 'Date'\n",
    "WEF_Model_data = pd.merge(\n",
    "    WEF_Model_streamflow_data,\n",
    "    WEF_Model_Climate_data,\n",
    "    on=['Station No.', 'Date'],\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Step 2: Merge the resulting DataFrame with WEF_Model_SE_data on 'Station No.' and 'Date'\n",
    "WEF_Model_data = pd.merge(\n",
    "    WEF_Model_data,\n",
    "    WEF_Model_SE_data,\n",
    "    on=['Station No.', 'Date'],\n",
    "    how='inner'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6324b0f-af02-4ec1-8aa6-94e9b300bdd1",
   "metadata": {
    "id": "e6324b0f-af02-4ec1-8aa6-94e9b300bdd1"
   },
   "outputs": [],
   "source": [
    "WEF_Model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d05c52c-0f73-4d9f-95f8-6f4c6418cb3e",
   "metadata": {
    "id": "0d05c52c-0f73-4d9f-95f8-6f4c6418cb3e"
   },
   "outputs": [],
   "source": [
    "# Add a \"Month\" column based on the \"Date\" column, right after the \"Flow rate (m3/s)\" column\n",
    "WEF_Model_data['Month'] = WEF_Model_data['Date'].dt.month\n",
    "\n",
    "# Rearrange the columns to place \"Month\" after \"Flow rate (m3/s)\"\n",
    "columns = list(WEF_Model_data.columns)\n",
    "# Insert \"Month\" right after \"Flow rate (m3/s)\"\n",
    "columns.insert(columns.index(\"Flow rate (m3/s)\") + 1, columns.pop(columns.index(\"Month\")))\n",
    "WEF_Model_data = WEF_Model_data[columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dca07c-40fb-4283-b49c-5fda64669e97",
   "metadata": {
    "id": "f0dca07c-40fb-4283-b49c-5fda64669e97"
   },
   "outputs": [],
   "source": [
    "# Add 'Area' column by merging with WEF_Model_info_data_07\n",
    "WEF_Model_data = WEF_Model_data.merge(\n",
    "    WEF_Model_info_data_07[['Station No.', 'Area_calc']],\n",
    "    on='Station No.',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rename 'Area_calc' to 'Area' after merging\n",
    "WEF_Model_data.rename(columns={'Area_calc': 'Area'}, inplace=True)\n",
    "\n",
    "# Add 'Elevation' column by merging with WEF_Model_info_data_07\n",
    "WEF_Model_data = WEF_Model_data.merge(\n",
    "    WEF_Model_info_data_07[['Station No.', 'Elev_calc']],\n",
    "    on='Station No.',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Rename 'Elev_calc' to 'Elevation' after merging\n",
    "WEF_Model_data.rename(columns={'Elev_calc': 'Elevation'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50cd436c-45d0-4765-8db7-1d525932697f",
   "metadata": {
    "id": "50cd436c-45d0-4765-8db7-1d525932697f"
   },
   "outputs": [],
   "source": [
    "WEF_Model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8221d5b7-f91e-4fb1-960e-b448734f14d8",
   "metadata": {
    "id": "8221d5b7-f91e-4fb1-960e-b448734f14d8"
   },
   "outputs": [],
   "source": [
    "print(WEF_Model_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9209f2-5c11-44ac-9365-7a523635fe59",
   "metadata": {
    "id": "6e9209f2-5c11-44ac-9365-7a523635fe59"
   },
   "outputs": [],
   "source": [
    "# WEF_Model_data.to_csv('WEF_Model007_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174c2a44-ef45-4331-ab7a-3bc62414acc8",
   "metadata": {
    "id": "174c2a44-ef45-4331-ab7a-3bc62414acc8"
   },
   "source": [
    "## Model dataset Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85af9d3b-b410-4cb4-a249-f02a1a0c5e77",
   "metadata": {
    "id": "85af9d3b-b410-4cb4-a249-f02a1a0c5e77"
   },
   "outputs": [],
   "source": [
    "# Get unique station numbers\n",
    "WEF_Model_stations = WEF_Model_data['Station No.'].unique()\n",
    "print(len(WEF_Model_stations))\n",
    "WEF_Model_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac35b7-c11e-4540-8961-034eafeea3db",
   "metadata": {
    "id": "6bac35b7-c11e-4540-8961-034eafeea3db"
   },
   "outputs": [],
   "source": [
    "# Group by 'Station No.' and calculate the mean\n",
    "stations_catchments_char = WEF_Model_data.groupby('Station No.')[WEF_Model_data.select_dtypes(include='number').columns].mean().reset_index()\n",
    "\n",
    "stations_catchments_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dda4275-6082-492c-b61d-1225c6581b24",
   "metadata": {
    "id": "7dda4275-6082-492c-b61d-1225c6581b24"
   },
   "outputs": [],
   "source": [
    "# stations_catchments_char.to_excel('stations_catchments_char.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5710555d-f206-4fe1-9d79-3ab0e1a9427e",
   "metadata": {
    "id": "5710555d-f206-4fe1-9d79-3ab0e1a9427e"
   },
   "outputs": [],
   "source": [
    "WEF_Model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68afb0c-0020-40ee-912f-0205211ce425",
   "metadata": {
    "id": "b68afb0c-0020-40ee-912f-0205211ce425"
   },
   "source": [
    "## Saved Model dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "l5fCgrVfjh_b",
   "metadata": {
    "id": "l5fCgrVfjh_b"
   },
   "outputs": [],
   "source": [
    "# WEF_Model_data.to_csv('/***/***/TUR Thesis Models (export)/WEF_Model_data.csv', index=False) # Replace with the actual path to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZBkWwf4Tos_-",
   "metadata": {
    "id": "ZBkWwf4Tos_-"
   },
   "outputs": [],
   "source": [
    "WEF_Model_data = pd.read_csv('/***/***/WEF_Model_data.csv') # Replace with the actual path to the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-HNwIFUhWZ1F",
   "metadata": {
    "id": "-HNwIFUhWZ1F"
   },
   "outputs": [],
   "source": [
    "WEF_Model_data['Date'] = pd.to_datetime(WEF_Model_data['Date']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LoKm-QQTWZtF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 652
    },
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1758846645476,
     "user": {
      "displayName": "Abdelrahman Habash",
      "userId": "17825699896667390000"
     },
     "user_tz": -180
    },
    "id": "LoKm-QQTWZtF",
    "outputId": "df2b2127-bb49-4f60-98db-693626daef34"
   },
   "outputs": [],
   "source": [
    "WEF_Model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2J-BSmmnjhJp",
   "metadata": {
    "id": "2J-BSmmnjhJp"
   },
   "source": [
    "## Training/Testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5576ac83-0a82-4af5-ae88-e5e38f81e14d",
   "metadata": {
    "id": "5576ac83-0a82-4af5-ae88-e5e38f81e14d"
   },
   "outputs": [],
   "source": [
    "# Create a dictionary to store train and test datasets for each station\n",
    "train_test_sets = {}\n",
    "\n",
    "# Define the date ranges\n",
    "train_start_date = datetime(2007, 10, 1)\n",
    "train_end_date = datetime(2017, 9, 30)\n",
    "test_start_date = datetime(2017, 10, 1)\n",
    "test_end_date = datetime(2020, 9, 30)\n",
    "\n",
    "# Iterate over unique stations\n",
    "for station in WEF_Model_data['Station No.'].unique():\n",
    "    # Define the test conditions for the specific station\n",
    "    test_conditions = (\n",
    "        (WEF_Model_data['Station No.'] == station) &\n",
    "        (WEF_Model_data['Date'] >= test_start_date) &\n",
    "        (WEF_Model_data['Date'] <= test_end_date)\n",
    "    )\n",
    "\n",
    "    # Define the train conditions for the specific station\n",
    "    train_conditions = (\n",
    "        (WEF_Model_data['Date'] >= train_start_date) &\n",
    "        (WEF_Model_data['Date'] <= train_end_date)\n",
    "    )\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    train_data = WEF_Model_data[train_conditions]\n",
    "    test_data = WEF_Model_data[test_conditions]\n",
    "\n",
    "    # Store the results in the dictionary\n",
    "    train_test_sets[station] = {\n",
    "        'train': train_data,\n",
    "        'test': test_data\n",
    "    }\n",
    "\n",
    "# Access the train and test sets for a specific station\n",
    "# example_station = 'D26A055'\n",
    "# WEF_Model_train_data_D26A055 = train_test_sets[example_station]['train']\n",
    "# WEF_Model_test_data_D26A055 = train_test_sets[example_station]['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe37b2a-9748-49b0-9c33-e23a2780ef34",
   "metadata": {
    "id": "5fe37b2a-9748-49b0-9c33-e23a2780ef34"
   },
   "source": [
    "## All Stations Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe2daf4-8aab-44d7-9d3e-3cb4a2411b82",
   "metadata": {
    "id": "fbe2daf4-8aab-44d7-9d3e-3cb4a2411b82"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# ðŸ’  Set random seed for reproducibility\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# # âš ï¸ Makes execution slower but deterministic\n",
    "# os.environ[\"TF_DETERMINISTIC_OPS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbd2c41-8d3a-4813-8562-431a434edd5d",
   "metadata": {
    "id": "6fbd2c41-8d3a-4813-8562-431a434edd5d"
   },
   "outputs": [],
   "source": [
    "test_dates = pd.date_range(start=test_start_date, end=test_end_date, freq='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1TT8IXqJO6Y6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1758885797967,
     "user": {
      "displayName": "Abdelrahman Habash",
      "userId": "17825699896667390000"
     },
     "user_tz": -180
    },
    "id": "1TT8IXqJO6Y6",
    "outputId": "64209645-06e9-4512-f4eb-19b5b7cb2bae"
   },
   "outputs": [],
   "source": [
    "# Group by 'Station No.' and calculate the mean\n",
    "stations_catchments_char = WEF_Model_data.groupby('Station No.')[WEF_Model_data.select_dtypes(include='number').columns].mean().reset_index()\n",
    "\n",
    "stations_catchments_char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3wGiIKlnONGb",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1758885797681,
     "user": {
      "displayName": "Abdelrahman Habash",
      "userId": "17825699896667390000"
     },
     "user_tz": -180
    },
    "id": "3wGiIKlnONGb",
    "outputId": "9c84ec96-787b-4a56-d6c7-238e14757460"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def get_area_class_bounds(area):\n",
    "    \"\"\"Returns fixed area class bounds based on REFORM-style intervals.\"\"\"\n",
    "    if area <= 10:\n",
    "        return 0, 10\n",
    "    elif area <= 100:\n",
    "        return 10, 100\n",
    "    elif area <= 1000:\n",
    "        return 100, 1000\n",
    "    elif area <= 10000:\n",
    "        return 1000, 10000\n",
    "    else:\n",
    "        return 10000, float('inf')\n",
    "\n",
    "def get_elevation_class_bounds(elevation):\n",
    "    \"\"\"Returns fixed elevation class bounds based on REFORM-style intervals.\"\"\"\n",
    "    if elevation < 200:\n",
    "        return float('-inf'), 200\n",
    "    elif elevation < 800:\n",
    "        return 200, 800\n",
    "    else:\n",
    "        return 800, float('inf')\n",
    "\n",
    "# List to store results\n",
    "results = []\n",
    "\n",
    "# Loop over each station in your station_characteristics DataFrame\n",
    "for idx, station in stations_catchments_char.iterrows():\n",
    "    station_id = station['Station No.']\n",
    "    area = station['Area']\n",
    "    elev = station['Elevation']\n",
    "\n",
    "    # Logarithmic bounds for area\n",
    "    try:\n",
    "        min_area, max_area = get_area_class_bounds(area)\n",
    "        min_elev, max_elev = get_elevation_class_bounds(elev)\n",
    "    except ValueError:\n",
    "        continue  # skip if invalid values\n",
    "\n",
    "    # Find relevant stations\n",
    "    filtered = stations_catchments_char[\n",
    "        (stations_catchments_char['Area'] >= min_area) &\n",
    "        (stations_catchments_char['Area'] <= max_area) &\n",
    "        (stations_catchments_char['Elevation'] >= min_elev) &\n",
    "        (stations_catchments_char['Elevation'] < max_elev)\n",
    "    ]\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        'Station No.': station_id,\n",
    "        'Relevant Stations count': len(filtered),\n",
    "        'Relevant Stations': filtered['Station No.'].tolist()\n",
    "    })\n",
    "\n",
    "# Convert to summary DataFrame\n",
    "RelSt_df = pd.DataFrame(results)\n",
    "\n",
    "# Display the summary\n",
    "print(RelSt_df)\n",
    "\n",
    "\n",
    "# Calculate summary statistics\n",
    "min_count = RelSt_df['Relevant Stations count'].min()\n",
    "max_count = RelSt_df['Relevant Stations count'].max()\n",
    "mean_count = RelSt_df['Relevant Stations count'].mean()\n",
    "mode_count = RelSt_df['Relevant Stations count'].mode().iloc[0]  # most frequent value\n",
    "\n",
    "# Print the statistics\n",
    "print(\"Summary Statistics for Relevant Stations count:\")\n",
    "print(f\"Minimum: {min_count}\")\n",
    "print(f\"Maximum: {max_count}\")\n",
    "print(f\"Mean: {mean_count:.2f}\")\n",
    "print(f\"Most Frequent (Mode): {mode_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac7c2f54-32b7-41b5-87b8-2098405d8c9c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1mD12EgYKrCvVNaw6JVS7oWovtV0QxGUp"
    },
    "executionInfo": {
     "elapsed": 266807,
     "status": "ok",
     "timestamp": 1758886346563,
     "user": {
      "displayName": "Abdelrahman Habash",
      "userId": "17825699896667390000"
     },
     "user_tz": -180
    },
    "id": "ac7c2f54-32b7-41b5-87b8-2098405d8c9c",
    "outputId": "eb957790-a0bc-4d3e-fb54-5964dbe8b70d"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import joblib\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Concatenate\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "window_size = 14\n",
    "\n",
    "# Define temporal and static features\n",
    "temporal_feature_names = ['Month',\n",
    "'u_component_of_wind_10m',\n",
    "'v_component_of_wind_10m',\n",
    "'dewpoint_temperature_2m',\n",
    "'temperature_2m',\n",
    "'snow_depth',\n",
    "'snow_density',\n",
    "'soil_temperature_level_1',\n",
    "'volumetric_soil_water_layer_1',\n",
    "'total_precipitation_sum',\n",
    "'total_evaporation_sum',\n",
    "'runoff_sum',\n",
    "'surface_runoff_sum',\n",
    "'sub_surface_runoff_sum',\n",
    "'surface_solar_radiation_downwards_sum',\n",
    "'surface_thermal_radiation_downwards_sum',\n",
    "'surface_net_solar_radiation_sum',\n",
    "'surface_net_thermal_radiation_sum',\n",
    "'surface_latent_heat_flux_sum',\n",
    "'surface_sensible_heat_flux_sum',\n",
    "]\n",
    "\n",
    "static_feature_names = ['Area', 'Elevation']\n",
    "\n",
    "# Create sliding windows for LSTM\n",
    "def create_sliding_windows(X, y, window_size):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(len(X) - window_size):\n",
    "        X_seq.append(X[i:i+window_size])\n",
    "        y_seq.append(y[i+window_size])\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# Define model\n",
    "class TwoBranchHybridModel:\n",
    "    def __init__(self, num_temporal_features, num_static_features, window_size):\n",
    "        self.temporal_input_dim = num_temporal_features\n",
    "        self.static_input_dim = num_static_features\n",
    "        self.window_size = window_size\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "\n",
    "        input_temporal = Input(shape=(self.window_size, self.temporal_input_dim), name=\"temporal_input\")\n",
    "        x = LSTM(64, return_sequences=True)(input_temporal)\n",
    "        x = LSTM(32)(x)\n",
    "        \n",
    "        input_static = Input(shape=(self.static_input_dim,), name=\"static_input\")\n",
    "        y = Dense(32, activation='relu')(input_static)\n",
    "        \n",
    "        combined = Concatenate()([x, y])\n",
    "        z = Dense(16, activation='relu')(combined)\n",
    "        output = Dense(1)(z)\n",
    "        \n",
    "        model = Model(inputs={'temporal_input': input_temporal, 'static_input': input_static}, outputs=output)\n",
    "        model.compile(loss='huber', optimizer=Adam(learning_rate=0.0001), metrics=['mae'])\n",
    "        \n",
    "        return model\n",
    "\n",
    "    def fit(self, X_temporal, X_static, y, **kwargs):\n",
    "        return self.model.fit({'temporal_input': X_temporal, 'static_input': X_static}, y, **kwargs)\n",
    "\n",
    "    def predict(self, X_temporal, X_static):\n",
    "        return self.model.predict({'temporal_input': X_temporal, 'static_input': X_static}).squeeze()\n",
    "\n",
    "    def summary(self):\n",
    "        return self.model.summary()\n",
    "\n",
    "# Load previous models if any\n",
    "if os.path.exists('/***/***/saved_WEF_Model_histories.joblib'): # Replace with the actual path to the file\n",
    "    saved_WEF_Model_histories = joblib.load('/***/***/saved_WEF_Model_histories.joblib') # Replace with the actual path to the file\n",
    "    print(\"ðŸ“‚ Loaded existing saved_WEF_Model_histories.\")\n",
    "else:\n",
    "    saved_WEF_Model_histories = {}\n",
    "    print(\"ðŸ“ No previous saved_WEF_Model_histories found. Starting fresh.\")\n",
    "\n",
    "# Main training loop\n",
    "for station in WEF_Model_data['Station No.'].unique():\n",
    "    if station in saved_WEF_Model_histories:\n",
    "        print(f\"\\nâœ… Skipping station {station} - Already processed. Plotting results.\")\n",
    "        plot_series(test_dates[window_size:],\n",
    "                    saved_WEF_Model_histories[station]['y_test'][window_size:],\n",
    "                    saved_WEF_Model_histories[station]['y_pred'],\n",
    "                    title=f\"{station} Model (Previously Trained)\")\n",
    "        continue\n",
    "\n",
    "    train_data = train_test_sets[station]['train'].copy()\n",
    "    test_data = train_test_sets[station]['test'].copy()\n",
    "\n",
    "    station_characteristics = stations_catchments_char[stations_catchments_char['Station No.'] == station]\n",
    "    if station_characteristics.empty:\n",
    "        print(f\"\\nâŒ Skipping station {station} - No characteristics found.\")\n",
    "        continue\n",
    "\n",
    "    relevant_stations_series = RelSt_df.loc[RelSt_df['Station No.'] == station, 'Relevant Stations']\n",
    "    if relevant_stations_series.empty:\n",
    "        print(f\"\\nâŒ Skipping station {station} - No relevant stations found for training.\")\n",
    "        continue\n",
    "\n",
    "    # Extract the actual list from the Series\n",
    "    relevant_stations = relevant_stations_series.iloc[0]\n",
    "\n",
    "    print(f\"\\nâœ… Station: {station} - Found {len(relevant_stations)} relevant stations.\")\n",
    "\n",
    "    train_data = train_data[train_data['Station No.'].isin(relevant_stations)]\n",
    "    test_data = test_data[test_data['Station No.'].isin(relevant_stations)]\n",
    "\n",
    "    if train_data.empty:\n",
    "        print(f\"\\nâŒ Skipping station {station} - No training data available after filtering.\")\n",
    "        continue\n",
    "\n",
    "    X_train = train_data.drop(columns=['Station No.', 'Date', 'Flow rate (m3/s)'])\n",
    "    y_train = train_data['Flow rate (m3/s)']\n",
    "    X_test = test_data.drop(columns=['Station No.', 'Date', 'Flow rate (m3/s)'])\n",
    "    y_test = test_data['Flow rate (m3/s)']\n",
    "\n",
    "    valid_idx = ~(X_train.isna().any(axis=1) | y_train.isna())\n",
    "    X_train, y_train = X_train[valid_idx], y_train[valid_idx]\n",
    "    valid_test_idx = ~(X_test.isna().any(axis=1) | y_test.isna())\n",
    "    X_test, y_test = X_test[valid_test_idx], y_test[valid_test_idx]\n",
    "\n",
    "    X_train_static = X_train[static_feature_names]\n",
    "    X_train_temporal = X_train[temporal_feature_names]\n",
    "    X_test_static = X_test[static_feature_names]\n",
    "    X_test_temporal = X_test[temporal_feature_names]\n",
    "\n",
    "    # Scale and reshape\n",
    "    scaler_static = StandardScaler()\n",
    "    scaler_temporal = StandardScaler()\n",
    "    X_train_static_scaled = scaler_static.fit_transform(X_train_static)\n",
    "    X_test_static_scaled = scaler_static.transform(X_test_static)\n",
    "    X_train_temporal_scaled = scaler_temporal.fit_transform(X_train_temporal)\n",
    "    X_test_temporal_scaled = scaler_temporal.transform(X_test_temporal)\n",
    "\n",
    "    X_train_temporal_seq, y_train_seq = create_sliding_windows(X_train_temporal_scaled, y_train.values, window_size)\n",
    "    X_test_temporal_seq, y_test_seq = create_sliding_windows(X_test_temporal_scaled, y_test.values, window_size)\n",
    "\n",
    "    X_train_static_seq = X_train_static_scaled[window_size:]\n",
    "    X_test_static_seq = X_test_static_scaled[window_size:]\n",
    "\n",
    "    model = TwoBranchHybridModel(\n",
    "        num_temporal_features=len(temporal_feature_names),\n",
    "        num_static_features=len(static_feature_names),\n",
    "        window_size=window_size\n",
    "    )\n",
    "\n",
    "    model.fit(X_train_temporal_seq, X_train_static_seq, y_train_seq, epochs=100, batch_size=128, verbose=1)\n",
    "    y_pred = model.predict(X_test_temporal_seq, X_test_static_seq)\n",
    "\n",
    "    saved_WEF_Model_histories[station] = {\n",
    "        'model': model,\n",
    "        'scaler_static': scaler_static,\n",
    "        'scaler_temporal': scaler_temporal,\n",
    "        'X_train': X_train,\n",
    "        'X_test': X_test,\n",
    "        'y_train': y_train,\n",
    "        'y_test': y_test,\n",
    "        'y_pred': y_pred,\n",
    "        'relevant_stations': relevant_stations,\n",
    "    }\n",
    "\n",
    "    joblib.dump(saved_WEF_Model_histories, '/***/***/saved_WEF_Model_histories.joblib') # Replace with the actual path to the file\n",
    "    print(f\"ðŸ’¾ Saved {station} to saved_WEF_Model_histories.joblib\")\n",
    "\n",
    "    plot_series(test_dates[window_size:], y_test_seq, y_pred, title=f\"{station} Model\")\n",
    "\n",
    "    del X_train_static, X_train_temporal, X_test_static, X_test_temporal\n",
    "    del X_train_static_scaled, X_train_temporal_scaled\n",
    "    del X_test_static_scaled, X_test_temporal_scaled\n",
    "    K.clear_session()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"âœ… Completed training and saving all stations to 'saved_WEF_Model_histories.joblib'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538b3c56-48ae-4b04-85e5-92cfd9b2c84a",
   "metadata": {
    "id": "538b3c56-48ae-4b04-85e5-92cfd9b2c84a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
